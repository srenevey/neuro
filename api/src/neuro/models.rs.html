<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source to the Rust file `src/models.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>models.rs.html -- source</title><link rel="stylesheet" type="text/css" href="../../normalize.css"><link rel="stylesheet" type="text/css" href="../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" type="text/css" href="../../dark.css"><link rel="stylesheet" type="text/css" href="../../light.css" id="themeStyle"><script src="../../storage.js"></script><noscript><link rel="stylesheet" href="../../noscript.css"></noscript><link rel="shortcut icon" href="../../favicon.ico"><style type="text/css">#crate-search{background-image:url("../../down-arrow.svg");}</style></head><body class="rustdoc source"><!--[if lte IE 8]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="sidebar"><div class="sidebar-menu">&#9776;</div><a href='../../neuro/index.html'><div class='logo-container'><img src='../../rust-logo.png' alt='logo'></div></a></nav><div class="theme-picker"><button id="theme-picker" aria-label="Pick another theme!"><img src="../../brush.svg" width="18" alt="Pick another theme!"></button><div id="theme-choices"></div></div><script src="../../theme.js"></script><nav class="sub"><form class="search-form js-only"><div class="search-container"><div><select id="crate-search"><option value="All crates">All crates</option></select><input class="search-input" name="search" autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"></div><a id="settings-menu" href="../../settings.html"><img src="../../wheel.svg" width="18" alt="Change settings"></a></div></form></nav><section id="main" class="content"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
<span id="316">316</span>
<span id="317">317</span>
<span id="318">318</span>
<span id="319">319</span>
<span id="320">320</span>
<span id="321">321</span>
<span id="322">322</span>
<span id="323">323</span>
<span id="324">324</span>
<span id="325">325</span>
<span id="326">326</span>
<span id="327">327</span>
<span id="328">328</span>
<span id="329">329</span>
<span id="330">330</span>
<span id="331">331</span>
<span id="332">332</span>
<span id="333">333</span>
<span id="334">334</span>
<span id="335">335</span>
<span id="336">336</span>
<span id="337">337</span>
<span id="338">338</span>
<span id="339">339</span>
<span id="340">340</span>
<span id="341">341</span>
<span id="342">342</span>
<span id="343">343</span>
<span id="344">344</span>
<span id="345">345</span>
<span id="346">346</span>
<span id="347">347</span>
<span id="348">348</span>
<span id="349">349</span>
<span id="350">350</span>
<span id="351">351</span>
<span id="352">352</span>
<span id="353">353</span>
<span id="354">354</span>
<span id="355">355</span>
<span id="356">356</span>
<span id="357">357</span>
<span id="358">358</span>
<span id="359">359</span>
<span id="360">360</span>
<span id="361">361</span>
<span id="362">362</span>
<span id="363">363</span>
<span id="364">364</span>
<span id="365">365</span>
<span id="366">366</span>
<span id="367">367</span>
<span id="368">368</span>
<span id="369">369</span>
<span id="370">370</span>
<span id="371">371</span>
<span id="372">372</span>
<span id="373">373</span>
<span id="374">374</span>
<span id="375">375</span>
<span id="376">376</span>
<span id="377">377</span>
<span id="378">378</span>
<span id="379">379</span>
<span id="380">380</span>
<span id="381">381</span>
<span id="382">382</span>
<span id="383">383</span>
<span id="384">384</span>
<span id="385">385</span>
<span id="386">386</span>
<span id="387">387</span>
<span id="388">388</span>
<span id="389">389</span>
<span id="390">390</span>
<span id="391">391</span>
<span id="392">392</span>
<span id="393">393</span>
<span id="394">394</span>
<span id="395">395</span>
<span id="396">396</span>
<span id="397">397</span>
<span id="398">398</span>
<span id="399">399</span>
<span id="400">400</span>
<span id="401">401</span>
<span id="402">402</span>
<span id="403">403</span>
<span id="404">404</span>
<span id="405">405</span>
<span id="406">406</span>
<span id="407">407</span>
<span id="408">408</span>
<span id="409">409</span>
<span id="410">410</span>
<span id="411">411</span>
<span id="412">412</span>
<span id="413">413</span>
<span id="414">414</span>
<span id="415">415</span>
<span id="416">416</span>
<span id="417">417</span>
<span id="418">418</span>
<span id="419">419</span>
<span id="420">420</span>
<span id="421">421</span>
<span id="422">422</span>
<span id="423">423</span>
<span id="424">424</span>
<span id="425">425</span>
<span id="426">426</span>
<span id="427">427</span>
<span id="428">428</span>
<span id="429">429</span>
<span id="430">430</span>
<span id="431">431</span>
<span id="432">432</span>
<span id="433">433</span>
<span id="434">434</span>
<span id="435">435</span>
<span id="436">436</span>
<span id="437">437</span>
<span id="438">438</span>
<span id="439">439</span>
<span id="440">440</span>
<span id="441">441</span>
<span id="442">442</span>
<span id="443">443</span>
<span id="444">444</span>
<span id="445">445</span>
<span id="446">446</span>
<span id="447">447</span>
<span id="448">448</span>
<span id="449">449</span>
<span id="450">450</span>
<span id="451">451</span>
<span id="452">452</span>
<span id="453">453</span>
<span id="454">454</span>
<span id="455">455</span>
<span id="456">456</span>
<span id="457">457</span>
<span id="458">458</span>
<span id="459">459</span>
<span id="460">460</span>
<span id="461">461</span>
<span id="462">462</span>
<span id="463">463</span>
<span id="464">464</span>
<span id="465">465</span>
<span id="466">466</span>
<span id="467">467</span>
<span id="468">468</span>
<span id="469">469</span>
<span id="470">470</span>
<span id="471">471</span>
<span id="472">472</span>
<span id="473">473</span>
<span id="474">474</span>
<span id="475">475</span>
<span id="476">476</span>
<span id="477">477</span>
<span id="478">478</span>
<span id="479">479</span>
<span id="480">480</span>
<span id="481">481</span>
<span id="482">482</span>
<span id="483">483</span>
<span id="484">484</span>
<span id="485">485</span>
<span id="486">486</span>
<span id="487">487</span>
<span id="488">488</span>
<span id="489">489</span>
<span id="490">490</span>
<span id="491">491</span>
<span id="492">492</span>
<span id="493">493</span>
<span id="494">494</span>
<span id="495">495</span>
<span id="496">496</span>
<span id="497">497</span>
<span id="498">498</span>
<span id="499">499</span>
<span id="500">500</span>
<span id="501">501</span>
<span id="502">502</span>
<span id="503">503</span>
<span id="504">504</span>
<span id="505">505</span>
<span id="506">506</span>
<span id="507">507</span>
<span id="508">508</span>
<span id="509">509</span>
<span id="510">510</span>
<span id="511">511</span>
<span id="512">512</span>
<span id="513">513</span>
<span id="514">514</span>
<span id="515">515</span>
<span id="516">516</span>
<span id="517">517</span>
<span id="518">518</span>
<span id="519">519</span>
<span id="520">520</span>
<span id="521">521</span>
<span id="522">522</span>
<span id="523">523</span>
<span id="524">524</span>
<span id="525">525</span>
<span id="526">526</span>
<span id="527">527</span>
<span id="528">528</span>
<span id="529">529</span>
<span id="530">530</span>
<span id="531">531</span>
<span id="532">532</span>
<span id="533">533</span>
<span id="534">534</span>
<span id="535">535</span>
<span id="536">536</span>
<span id="537">537</span>
<span id="538">538</span>
<span id="539">539</span>
<span id="540">540</span>
<span id="541">541</span>
<span id="542">542</span>
<span id="543">543</span>
<span id="544">544</span>
<span id="545">545</span>
<span id="546">546</span>
<span id="547">547</span>
<span id="548">548</span>
<span id="549">549</span>
<span id="550">550</span>
<span id="551">551</span>
<span id="552">552</span>
<span id="553">553</span>
<span id="554">554</span>
<span id="555">555</span>
<span id="556">556</span>
<span id="557">557</span>
<span id="558">558</span>
<span id="559">559</span>
<span id="560">560</span>
<span id="561">561</span>
<span id="562">562</span>
<span id="563">563</span>
<span id="564">564</span>
<span id="565">565</span>
<span id="566">566</span>
<span id="567">567</span>
<span id="568">568</span>
<span id="569">569</span>
<span id="570">570</span>
<span id="571">571</span>
<span id="572">572</span>
<span id="573">573</span>
<span id="574">574</span>
<span id="575">575</span>
<span id="576">576</span>
<span id="577">577</span>
</pre><div class="example-wrap"><pre class="rust ">
<span class="doccomment">//! Base module to create neural networks.</span>
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">data</span>::{<span class="ident">DataSet</span>, <span class="ident">BatchIterator</span>, <span class="ident">Scaling</span>};
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">layers</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">losses</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">metrics</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">optimizers</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">regularizers</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">tensor</span>::<span class="kw-2">*</span>;

<span class="kw">use</span> <span class="ident">std</span>::<span class="ident">fmt</span>;
<span class="kw">use</span> <span class="ident">std</span>::<span class="ident">fs</span>;
<span class="kw">use</span> <span class="ident">std</span>::<span class="ident">io</span>;
<span class="kw">use</span> <span class="ident">std</span>::<span class="ident">io</span>::{<span class="ident">BufWriter</span>, <span class="ident">BufReader</span>, <span class="ident">Read</span>, <span class="ident">Write</span>};
<span class="kw">use</span> <span class="ident">std</span>::<span class="ident">path</span>::<span class="ident">Path</span>;

<span class="kw">use</span> <span class="ident">arrayfire</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="ident">indicatif</span>::{<span class="ident">ProgressBar</span>, <span class="ident">ProgressStyle</span>};

<span class="kw">enum</span> <span class="ident">Mode</span> {
    <span class="ident">Test</span>,
    <span class="ident">Train</span>,
    <span class="ident">Valid</span>,
}

<span class="doccomment">/// Structure representing a neural network.</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">Network</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="ident">D</span>, <span class="ident">O</span>, <span class="ident">L</span><span class="op">&gt;</span>
<span class="kw">where</span> <span class="ident">D</span>: <span class="ident">DataSet</span>, <span class="ident">O</span>: <span class="ident">Optimizer</span>, <span class="ident">L</span>: <span class="ident">Loss</span>
{
    <span class="ident">data</span>: <span class="kw-2">&amp;</span><span class="lifetime">&#39;a</span> <span class="ident">D</span>,
    <span class="ident">layers</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">Box</span><span class="op">&lt;</span><span class="kw">dyn</span> <span class="ident">Layer</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    <span class="ident">loss_function</span>: <span class="ident">L</span>,
    <span class="ident">optimizer</span>: <span class="ident">O</span>,
    <span class="ident">regularizer</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Regularizer</span><span class="op">&gt;</span>,
    <span class="ident">input_shape</span>: <span class="ident">Dim4</span>,
    <span class="ident">output_shape</span>: <span class="ident">Dim4</span>,
    <span class="ident">classes</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">String</span><span class="op">&gt;</span><span class="op">&gt;</span>,
}


<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="ident">D</span>, <span class="ident">O</span>, <span class="ident">L</span><span class="op">&gt;</span> <span class="ident">Network</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="ident">D</span>, <span class="ident">O</span>, <span class="ident">L</span><span class="op">&gt;</span>
<span class="kw">where</span> <span class="ident">D</span>: <span class="ident">DataSet</span>, <span class="ident">O</span>: <span class="ident">Optimizer</span>, <span class="ident">L</span>: <span class="ident">Loss</span>
{
    <span class="doccomment">/// Creates an empty neural network.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `data`: dataset used to train the neural network</span>
    <span class="doccomment">/// * `loss_function`: loss function minimized in the optimization process</span>
    <span class="doccomment">/// * `optimizer`: algorithm used to optimize the parameters of the network</span>
    <span class="doccomment">/// * `regularizer`: (optional) method used to regularize the model</span>
    <span class="doccomment">///</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">new</span>(<span class="ident">data</span>: <span class="kw-2">&amp;</span><span class="lifetime">&#39;a</span> <span class="ident">D</span>,
               <span class="ident">loss_function</span>: <span class="ident">L</span>,
               <span class="ident">optimizer</span>: <span class="ident">O</span>,
               <span class="ident">regularizer</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Regularizer</span><span class="op">&gt;</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Network</span><span class="op">&lt;</span><span class="ident">D</span>, <span class="ident">O</span>, <span class="ident">L</span><span class="op">&gt;</span> {
        <span class="ident">Network</span> {
            <span class="ident">data</span>,
            <span class="ident">layers</span>: <span class="ident">Vec</span>::<span class="ident">new</span>(),
            <span class="ident">loss_function</span>,
            <span class="ident">optimizer</span>,
            <span class="ident">regularizer</span>,
            <span class="ident">input_shape</span>: <span class="ident">data</span>.<span class="ident">input_shape</span>(),
            <span class="ident">output_shape</span>: <span class="ident">data</span>.<span class="ident">output_shape</span>(),
            <span class="ident">classes</span>: <span class="ident">data</span>.<span class="ident">classes</span>(),
        }
    }

    <span class="doccomment">/// Adds a layer to the network.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `layer`: layer to be added</span>
    <span class="doccomment">///</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">add</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">self</span>, <span class="ident">layer</span>: <span class="ident">Box</span><span class="op">&lt;</span><span class="kw">dyn</span> <span class="ident">Layer</span><span class="op">&gt;</span>) {
        <span class="kw">let</span> <span class="ident">input_shape</span> <span class="op">=</span> <span class="kw">match</span> <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">last</span>() {
            <span class="prelude-val">Some</span>(<span class="ident">layer</span>) <span class="op">=</span><span class="op">&gt;</span> <span class="ident">layer</span>.<span class="ident">output_shape</span>(),
            <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> <span class="self">self</span>.<span class="ident">data</span>.<span class="ident">input_shape</span>(),
        };
        <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">push</span>(<span class="ident">layer</span>);
        <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">last_mut</span>().<span class="ident">unwrap</span>().<span class="ident">initialize_parameters</span>(<span class="ident">input_shape</span>);
        <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">last_mut</span>().<span class="ident">unwrap</span>().<span class="ident">set_regularizer</span>(<span class="self">self</span>.<span class="ident">regularizer</span>);
    }


    <span class="doccomment">/// Computes the output of the network for a given input.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `input`: array containing the samples to be evaluated</span>
    <span class="doccomment">///</span>
    <span class="kw">fn</span> <span class="ident">forward</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">input</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Tensor</span> {
        <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">iter</span>().<span class="ident">fold</span>(
            <span class="ident">input</span>.<span class="ident">copy</span>(),
            <span class="op">|</span><span class="ident">a_prev</span>, <span class="ident">layer</span><span class="op">|</span> <span class="ident">layer</span>.<span class="ident">compute_activation</span>(<span class="kw-2">&amp;</span><span class="ident">a_prev</span>)
        )
    }


    <span class="doccomment">/// Computes a forward pass of the network.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// The intermediate linear activations computed during the forward pass are stored in each layer for efficient back propagation.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `input`: array containing the samples fed into the model</span>
    <span class="doccomment">///</span>
    <span class="kw">fn</span> <span class="ident">forward_mut</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">self</span>, <span class="ident">input</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">Tensor</span>) {
        <span class="kw">for</span> <span class="ident">layer</span> <span class="kw">in</span> <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">iter_mut</span>() {
            <span class="kw-2">*</span><span class="ident">input</span> <span class="op">=</span> <span class="ident">layer</span>.<span class="ident">compute_activation_mut</span>(<span class="ident">input</span>);
        }
    }

    <span class="doccomment">/// Computes a backward pass of the network.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `y_pred`: output of the network</span>
    <span class="doccomment">/// * `y_true`: true labels</span>
    <span class="doccomment">///</span>
    <span class="kw">fn</span> <span class="ident">backward</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">self</span>,
                <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
                <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) {
        <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">iter_mut</span>().<span class="ident">rev</span>().<span class="ident">fold</span>(
            <span class="self">self</span>.<span class="ident">loss_function</span>.<span class="ident">grad</span>(<span class="ident">y_pred</span>, <span class="ident">y_true</span>),
            <span class="op">|</span><span class="ident">da_prev</span>, <span class="ident">layer</span><span class="op">|</span> <span class="ident">layer</span>.<span class="ident">compute_dactivation_mut</span>(<span class="kw-2">&amp;</span><span class="ident">da_prev</span>)
        );

        <span class="comment">/*
        let mut da_prev = self.loss_function.grad(y_pred, y_true);
        for layer in self.layers.iter_mut().rev() {
            da_prev = layer.compute_da_prev_mut(&amp;da_prev);
        }
        */</span>
    }

    <span class="doccomment">/// Fits the neural network with the training data.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// The training data are shuffled at the beginning of each epoch, before batches are created.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `batch_size`: size of the mini-batches used for training</span>
    <span class="doccomment">/// * `epochs`: number of epochs to train for</span>
    <span class="doccomment">/// * `print_loss`: number of epochs between two computations and printing of the validation loss</span>
    <span class="doccomment">///</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">fit</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">self</span>,
               <span class="ident">batch_size</span>: <span class="ident">u64</span>,
               <span class="ident">epochs</span>: <span class="ident">u64</span>,
               <span class="ident">print_loss</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">u64</span><span class="op">&gt;</span>,
               <span class="ident">metrics</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">Metrics</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    ) {
        <span class="kw">let</span> <span class="ident">device</span> <span class="op">=</span> <span class="ident">get_device</span>();
        <span class="kw">let</span> (<span class="ident">name</span>, <span class="ident">platform</span>, <span class="kw">_</span>, <span class="kw">_</span>) <span class="op">=</span> <span class="ident">device_info</span>();
        <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;Running on {} using {}.&quot;</span>, <span class="ident">name</span>, <span class="ident">platform</span>);

        <span class="self">self</span>.<span class="ident">initialize_optimizer</span>();

        <span class="comment">// Initialize progress bar</span>
        <span class="kw">let</span> <span class="ident">num_bins</span> <span class="op">=</span> <span class="kw">match</span> <span class="ident">print_loss</span> {
            <span class="prelude-val">Some</span>(<span class="ident">p</span>) <span class="op">=</span><span class="op">&gt;</span> {
                <span class="kw">let</span> <span class="ident">num_batches_train</span> <span class="op">=</span> <span class="number">2</span> <span class="op">*</span> <span class="ident">p</span> <span class="op">*</span> (<span class="self">self</span>.<span class="ident">data</span>.<span class="ident">num_train_samples</span>() <span class="kw">as</span> <span class="ident">f64</span> <span class="op">/</span> <span class="ident">batch_size</span> <span class="kw">as</span> <span class="ident">f64</span>).<span class="ident">ceil</span>() <span class="kw">as</span> <span class="ident">u64</span>;
                <span class="kw">let</span> <span class="ident">num_batches_valid</span> <span class="op">=</span> (<span class="self">self</span>.<span class="ident">data</span>.<span class="ident">num_valid_samples</span>() <span class="kw">as</span> <span class="ident">f64</span> <span class="op">/</span> <span class="ident">batch_size</span> <span class="kw">as</span> <span class="ident">f64</span>).<span class="ident">ceil</span>() <span class="kw">as</span> <span class="ident">u64</span>;
                <span class="ident">num_batches_train</span> <span class="op">+</span> <span class="ident">num_batches_valid</span>
            },
            <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> <span class="ident">epochs</span>
        };
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">progress_bar</span> <span class="op">=</span> <span class="ident">ProgressBar</span>::<span class="ident">new</span>(<span class="ident">num_bins</span>);
        <span class="kw">let</span> <span class="ident">sty</span> <span class="op">=</span> <span class="ident">ProgressStyle</span>::<span class="ident">default_bar</span>()
            .<span class="ident">template</span>(<span class="string">&quot;[{elapsed_precise}] [{bar:50}] {msg}&quot;</span>)
            .<span class="ident">progress_chars</span>(<span class="string">&quot;##-&quot;</span>);
        <span class="ident">progress_bar</span>.<span class="ident">set_style</span>(<span class="ident">sty</span>.<span class="ident">clone</span>());


        <span class="kw">for</span> <span class="ident">epoch</span> <span class="kw">in</span> <span class="number">1</span>..<span class="op">=</span><span class="ident">epochs</span> {
            <span class="kw">let</span> (<span class="ident">x_train_shuffled</span>, <span class="ident">y_train_shuffled</span>) <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">shuffle</span>(<span class="self">self</span>.<span class="ident">data</span>.<span class="ident">x_train</span>(), <span class="self">self</span>.<span class="ident">data</span>.<span class="ident">y_train</span>());
            <span class="kw">let</span> <span class="ident">batches</span> <span class="op">=</span> <span class="ident">BatchIterator</span>::<span class="ident">new</span>((<span class="kw-2">&amp;</span><span class="ident">x_train_shuffled</span>, <span class="kw-2">&amp;</span><span class="ident">y_train_shuffled</span>), <span class="ident">batch_size</span>);

            <span class="comment">// Reset progress bar</span>
            <span class="kw">if</span> <span class="ident">progress_bar</span>.<span class="ident">is_finished</span>() {
                <span class="ident">progress_bar</span> <span class="op">=</span> <span class="ident">ProgressBar</span>::<span class="ident">new</span>(<span class="ident">num_bins</span>);
                <span class="ident">progress_bar</span>.<span class="ident">set_style</span>(<span class="ident">sty</span>.<span class="ident">clone</span>());
            }
            <span class="ident">progress_bar</span>.<span class="ident">set_message</span>(<span class="kw-2">&amp;</span><span class="macro">format</span><span class="macro">!</span>(<span class="string">&quot;epoch: {}/{}&quot;</span>, <span class="ident">epoch</span>, <span class="ident">epochs</span>));


            <span class="comment">// Iterate over the batches</span>
            <span class="kw">for</span> (<span class="kw-2">mut</span> <span class="ident">mini_batch_x</span>, <span class="ident">mini_batch_y</span>) <span class="kw">in</span> <span class="ident">batches</span> {

                <span class="comment">// Compute a pass on the network</span>
                <span class="self">self</span>.<span class="ident">forward_mut</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">mini_batch_x</span>);
                <span class="self">self</span>.<span class="ident">backward</span>(<span class="kw-2">&amp;</span><span class="ident">mini_batch_x</span>, <span class="kw-2">&amp;</span><span class="ident">mini_batch_y</span>);

                <span class="comment">// Update the parameters of the model</span>
                <span class="self">self</span>.<span class="ident">update_parameters</span>();

                <span class="ident">sync</span>(<span class="ident">device</span>);
                <span class="ident">progress_bar</span>.<span class="ident">inc</span>(<span class="number">1</span>);
            }

            <span class="comment">// Compute and print the losses and the metrics</span>
            <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">print_iter</span>) <span class="op">=</span> <span class="ident">print_loss</span> {
                <span class="kw">if</span> <span class="ident">epoch</span> <span class="op">%</span> <span class="ident">print_iter</span> <span class="op">=</span><span class="op">=</span> <span class="number">0</span> {

                    <span class="comment">// Compute the loss and metrics on the training set</span>
                    <span class="kw">let</span> (<span class="ident">train_loss</span>, <span class="ident">train_pred</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">compute_loss</span>(<span class="ident">batch_size</span>, <span class="ident">Mode</span>::<span class="ident">Train</span>, <span class="prelude-val">Some</span>(<span class="kw-2">&amp;</span><span class="ident">progress_bar</span>));
                    <span class="kw">let</span> <span class="ident">train_metrics_values</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">compute_metrics</span>(<span class="kw-2">&amp;</span><span class="ident">train_pred</span>, <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">data</span>.<span class="ident">y_train</span>(), <span class="ident">batch_size</span>, <span class="kw-2">&amp;</span><span class="ident">metrics</span>);

                    <span class="comment">// Compute the loss and metrics on the validation set</span>
                    <span class="kw">let</span> (<span class="ident">valid_loss</span>, <span class="ident">valid_pred</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">compute_loss</span>(<span class="ident">batch_size</span>, <span class="ident">Mode</span>::<span class="ident">Valid</span>, <span class="prelude-val">Some</span>(<span class="kw-2">&amp;</span><span class="ident">progress_bar</span>));
                    <span class="kw">let</span> <span class="ident">valid_metrics_values</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">compute_metrics</span>(<span class="kw-2">&amp;</span><span class="ident">valid_pred</span>, <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">data</span>.<span class="ident">y_valid</span>(), <span class="ident">batch_size</span>, <span class="kw-2">&amp;</span><span class="ident">metrics</span>);

                    <span class="ident">progress_bar</span>.<span class="ident">finish_with_message</span>(<span class="kw-2">&amp;</span><span class="macro">format</span><span class="macro">!</span>(<span class="string">&quot;epoch: {}/{}, train_loss: {}, train_metrics: {:?}, valid_loss: {}, valid_metrics: {:?}&quot;</span>, <span class="ident">epoch</span>, <span class="ident">epochs</span>, <span class="ident">train_loss</span>, <span class="ident">train_metrics_values</span>, <span class="ident">valid_loss</span>, <span class="ident">valid_metrics_values</span>));
                }
            }
        }
    }


    <span class="doccomment">/// Initializes the parameters of the optimizer.</span>
    <span class="kw">fn</span> <span class="ident">initialize_optimizer</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">self</span>) {
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">dims</span> <span class="op">=</span> <span class="ident">Vec</span>::<span class="op">&lt;</span>(<span class="ident">Dim4</span>, <span class="ident">Dim4</span>)<span class="op">&gt;</span>::<span class="ident">new</span>();
        <span class="kw">for</span> <span class="ident">layer</span> <span class="kw">in</span> <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">iter</span>() {
            <span class="kw">match</span> <span class="ident">layer</span>.<span class="ident">parameters</span>() {
                <span class="prelude-val">Some</span>(<span class="ident">param</span>) <span class="op">=</span><span class="op">&gt;</span> <span class="ident">dims</span>.<span class="ident">push</span>((<span class="ident">param</span>[<span class="number">0</span>].<span class="ident">dims</span>(), <span class="ident">param</span>[<span class="number">1</span>].<span class="ident">dims</span>())),
                <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> <span class="ident">dims</span>.<span class="ident">push</span>((<span class="ident">Dim4</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]), <span class="ident">Dim4</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])))
            }

        }
        <span class="self">self</span>.<span class="ident">optimizer</span>.<span class="ident">initialize_opt_params</span>(<span class="ident">dims</span>);
    }


    <span class="comment">/*
    /// Computes the loss for the predicted output.
    ///
    /// # Arguments
    /// * `y_pred`: predicted output of the model
    /// * `y_true`: true labels
    ///
    fn compute_loss(&amp;self,
                    y_pred: &amp;Tensor,
                    y_true: &amp;Tensor
    ) -&gt; PrimitiveType {
        let regularization = match &amp;self.regularizer {
            Some(regularizer) =&gt; {
                let mut weights: Vec&lt;&amp;Tensor&gt; = Vec::new();
                for layer in self.layers.iter() {
                    match layer.parameters() {
                        Some(params) =&gt; weights.push(params[0]),
                        None =&gt; {}
                    }
                }
                regularizer.eval(weights)
            },
            None =&gt; 0.0,
        };
        self.loss_function.eval(y_pred, y_true) + regularization
    }
    */</span>

    <span class="doccomment">/// Computes the loss and the predicted output.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `batch_size`: size of the mini-batches used to compute the loss</span>
    <span class="doccomment">/// * `mode`: specifies if the loss is computed on the training, validation, or test set</span>
    <span class="doccomment">/// * `bar`: (optional) reference to a progress bar used to show training progress</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Returns</span>
    <span class="doccomment">/// Tuple containing the loss and the predicted output.</span>
    <span class="doccomment">///</span>
    <span class="kw">fn</span> <span class="ident">compute_loss</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
                    <span class="ident">batch_size</span>: <span class="ident">u64</span>,
                    <span class="ident">mode</span>: <span class="ident">Mode</span>,
                    <span class="ident">progress_bar</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">ProgressBar</span><span class="op">&gt;</span>
    ) <span class="op">-</span><span class="op">&gt;</span> (<span class="ident">PrimitiveType</span>, <span class="ident">Tensor</span>) {
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">loss</span> <span class="op">=</span> <span class="number">0.</span>;
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">y_pred</span> <span class="op">=</span> <span class="ident">Array</span>::<span class="ident">new_empty</span>(<span class="self">self</span>.<span class="ident">output_shape</span>);

        <span class="comment">// Create batch iterator</span>
        <span class="kw">let</span> (<span class="ident">x</span>, <span class="ident">y</span>) <span class="op">=</span> <span class="kw">match</span> <span class="ident">mode</span> {
            <span class="ident">Mode</span>::<span class="ident">Train</span> <span class="op">=</span><span class="op">&gt;</span> (<span class="self">self</span>.<span class="ident">data</span>.<span class="ident">x_train</span>(), <span class="self">self</span>.<span class="ident">data</span>.<span class="ident">y_train</span>()),
            <span class="ident">Mode</span>::<span class="ident">Valid</span> <span class="op">=</span><span class="op">&gt;</span> (<span class="self">self</span>.<span class="ident">data</span>.<span class="ident">x_valid</span>(), <span class="self">self</span>.<span class="ident">data</span>.<span class="ident">y_valid</span>()),
            <span class="ident">Mode</span>::<span class="ident">Test</span> <span class="op">=</span><span class="op">&gt;</span> (<span class="self">self</span>.<span class="ident">data</span>.<span class="ident">x_test</span>().<span class="ident">expect</span>(<span class="string">&quot;No test samples have been provided.&quot;</span>), <span class="self">self</span>.<span class="ident">data</span>.<span class="ident">y_test</span>().<span class="ident">expect</span>(<span class="string">&quot;No test labels have been provided.&quot;</span>)),
        };
        <span class="kw">let</span> <span class="ident">batches</span> <span class="op">=</span> <span class="ident">BatchIterator</span>::<span class="ident">new</span>((<span class="ident">x</span>, <span class="ident">y</span>), <span class="ident">batch_size</span>);
        <span class="kw">let</span> <span class="ident">num_batches</span> <span class="op">=</span> <span class="ident">batches</span>.<span class="ident">num_batches</span>() <span class="kw">as</span> <span class="ident">PrimitiveType</span>;

        <span class="kw">for</span> (<span class="ident">count</span>, (<span class="ident">mini_batch_x</span>, <span class="ident">mini_batch_y</span>)) <span class="kw">in</span> <span class="ident">batches</span>.<span class="ident">enumerate</span>() {
            <span class="kw">let</span> <span class="ident">y_pred_batch</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">forward</span>(<span class="kw-2">&amp;</span><span class="ident">mini_batch_x</span>);

            <span class="kw">let</span> <span class="ident">regularization</span> <span class="op">=</span> <span class="kw">match</span> <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">regularizer</span> {
                <span class="prelude-val">Some</span>(<span class="ident">regularizer</span>) <span class="op">=</span><span class="op">&gt;</span> {
                    <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">weights</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="kw-2">&amp;</span><span class="ident">Tensor</span><span class="op">&gt;</span> <span class="op">=</span> <span class="ident">Vec</span>::<span class="ident">new</span>();
                    <span class="kw">for</span> <span class="ident">layer</span> <span class="kw">in</span> <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">iter</span>() {
                        <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">parameters</span>) <span class="op">=</span> <span class="ident">layer</span>.<span class="ident">parameters</span>() { <span class="ident">weights</span>.<span class="ident">push</span>(<span class="ident">parameters</span>[<span class="number">0</span>]) }
                    }
                    <span class="ident">regularizer</span>.<span class="ident">eval</span>(<span class="ident">weights</span>)
                },
                <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> <span class="number">0.0</span>,
            };
            <span class="ident">loss</span> <span class="op">+</span><span class="op">=</span> <span class="self">self</span>.<span class="ident">loss_function</span>.<span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="ident">y_pred_batch</span>, <span class="kw-2">&amp;</span><span class="ident">mini_batch_y</span>) <span class="op">+</span> <span class="ident">regularization</span>;

            <span class="kw">if</span> <span class="ident">count</span> <span class="op">=</span><span class="op">=</span> <span class="number">0</span> {
                <span class="ident">y_pred</span> <span class="op">=</span> <span class="ident">y_pred_batch</span>;
            } <span class="kw">else</span> {
                <span class="ident">y_pred</span> <span class="op">=</span> <span class="ident">join</span>(<span class="number">3</span>, <span class="kw-2">&amp;</span><span class="ident">y_pred</span>, <span class="kw-2">&amp;</span><span class="ident">y_pred_batch</span>);
            }

            <span class="kw">if</span> <span class="kw">let</span> <span class="prelude-val">Some</span>(<span class="ident">progress_bar</span>) <span class="op">=</span> <span class="ident">progress_bar</span> { <span class="ident">progress_bar</span>.<span class="ident">inc</span>(<span class="number">1</span>) }
        }
        (<span class="ident">loss</span> <span class="op">/</span> <span class="ident">num_batches</span>, <span class="ident">y_pred</span>)
    }


    <span class="comment">/*
    /// Computes the loss on the training set.
    ///
    /// # Arguments
    /// * `batch_size`: size of the mini-batches used to propagate the training data
    ///
    fn compute_train_loss(&amp;self, batch_size: u64, bar: &amp;ProgressBar) -&gt; PrimitiveType {
        let mut loss = 0.;

        // Create batch iterator
        let batches = BatchIterator::new((self.data.x_train(), self.data.y_train()), batch_size);
        let num_batches = batches.num_batches() as PrimitiveType;
        for (mini_batch_x, mini_batch_y) in batches {
            let y_pred_batch = self.forward(&amp;mini_batch_x);
            loss += self.compute_loss(&amp;y_pred_batch, &amp;mini_batch_y);
            bar.inc(1);
        }
        loss / num_batches
    }


    /// Evaluates the model with the validation data.
    ///
    /// # Returns
    /// Returns the loss computed on the validation set and the predictions in a tuple.
    ///
    fn validate(&amp;self, batch_size: u64, bar: &amp;ProgressBar) -&gt; (PrimitiveType, Tensor) {
        let mut loss = 0.;
        let mut y_pred = Array::new_empty(self.output_shape);

        // Create batch iterator
        let batches = BatchIterator::new((self.data.x_valid(), self.data.y_valid()), batch_size);
        let num_batches = batches.num_batches() as PrimitiveType;
        let mut count = 0;
        for (mini_batch_x, mini_batch_y) in batches {
            let y_pred_batch = self.forward(&amp;mini_batch_x);
            loss += self.compute_loss(&amp;y_pred_batch, &amp;mini_batch_y);

            if count == 0 {
                y_pred = y_pred_batch;
            } else {
                y_pred = join(3, &amp;y_pred, &amp;y_pred_batch);
            }
            count += 1;
            bar.inc(1);
        }
        (loss / num_batches, y_pred)
    }
    */</span>

    <span class="doccomment">/// Evaluates the model on the test set.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `metrics`: vector containing the metrics that will be evaluated</span>
    <span class="doccomment">///</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">evaluate</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">metrics</span>: <span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">Metrics</span><span class="op">&gt;</span><span class="op">&gt;</span>) {
        <span class="kw">let</span> (<span class="ident">loss</span>, <span class="ident">y_pred</span>) <span class="op">=</span> <span class="self">self</span>.<span class="ident">compute_loss</span>(<span class="number">128</span>, <span class="ident">Mode</span>::<span class="ident">Test</span>, <span class="prelude-val">None</span>);
        <span class="kw">let</span> <span class="ident">y_test</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">data</span>.<span class="ident">y_test</span>().<span class="ident">expect</span>(<span class="string">&quot;No test labels have been provided.&quot;</span>);
        <span class="comment">/*
        let mut loss = 0.;
        let mut y_pred = Array::new_empty(self.output_shape);

        let x_test = self.data.x_test().expect(&quot;No test samples have been provided.&quot;);
        let y_test = self.data.y_test().expect(&quot;No test labels have been provided.&quot;);

        // Create batch iterator
        let batches = BatchIterator::new((x_test, y_test), 128);
        let num_batches = batches.num_batches() as PrimitiveType;
        let mut count = 0;
        for (mini_batch_x, mini_batch_y) in batches {
            let y_pred_batch = self.forward(&amp;mini_batch_x);
            loss += self.compute_loss(&amp;y_pred_batch, &amp;mini_batch_y);

            if count == 0 {
                y_pred = y_pred_batch;
            } else {
                y_pred = join(3, &amp;y_pred, &amp;y_pred_batch);
            }
            count += 1;
        }
        loss /= num_batches;
        */</span>
        <span class="kw">let</span> <span class="ident">metrics_values</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">compute_metrics</span>(<span class="kw-2">&amp;</span><span class="ident">y_pred</span>, <span class="ident">y_test</span>, <span class="number">128</span>, <span class="kw-2">&amp;</span><span class="ident">metrics</span>);
        <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;Evaluation of the test set: loss: {}, metrics: {:?}&quot;</span>, <span class="ident">loss</span>, <span class="ident">metrics_values</span>);
    }



    <span class="doccomment">/// Evaluates the metrics.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `y_pred`: labels predicted by the model</span>
    <span class="doccomment">/// * `y_true`: true labels</span>
    <span class="doccomment">/// * `batch_size`: y_pred and y_true are split in chunks of batch_size to reduce the memory footprint</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Returns</span>
    <span class="doccomment">/// Vector containing the values for each metric.</span>
    <span class="doccomment">///</span>
    <span class="kw">fn</span> <span class="ident">compute_metrics</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
                       <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
                       <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
                       <span class="ident">batch_size</span>: <span class="ident">u64</span>,
                       <span class="ident">metrics</span>: <span class="kw-2">&amp;</span><span class="prelude-ty">Option</span><span class="op">&lt;</span><span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">Metrics</span><span class="op">&gt;</span><span class="op">&gt;</span>,
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">PrimitiveType</span><span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">num_metrics</span> <span class="op">=</span> <span class="kw">match</span> <span class="ident">metrics</span> {
            <span class="prelude-val">Some</span>(<span class="ident">m</span>) <span class="op">=</span><span class="op">&gt;</span> <span class="ident">m</span>.<span class="ident">len</span>(),
            <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> <span class="number">0</span>
        };

        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">metrics_values</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">PrimitiveType</span><span class="op">&gt;</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="number">0.</span>; <span class="ident">num_metrics</span>];

        <span class="kw">match</span> <span class="ident">metrics</span> {
            <span class="prelude-val">Some</span>(<span class="ident">m</span>) <span class="op">=</span><span class="op">&gt;</span> {
                <span class="kw">let</span> <span class="ident">batches</span> <span class="op">=</span> <span class="ident">BatchIterator</span>::<span class="ident">new</span>((<span class="ident">y_pred</span>, <span class="ident">y_true</span>), <span class="ident">batch_size</span>);
                <span class="kw">let</span> <span class="ident">num_batches</span> <span class="op">=</span> <span class="ident">batches</span>.<span class="ident">num_batches</span>() <span class="kw">as</span> <span class="ident">PrimitiveType</span>;

                <span class="kw">for</span> (<span class="ident">y_pred_batch</span>, <span class="ident">y_true_batch</span>) <span class="kw">in</span> <span class="ident">batches</span> {
                    <span class="kw">for</span> (<span class="ident">i</span>, <span class="ident">metrics</span>) <span class="kw">in</span> <span class="ident">m</span>.<span class="ident">iter</span>().<span class="ident">enumerate</span>() {
                        <span class="kw">let</span> <span class="ident">metrics_value</span> <span class="op">=</span> <span class="ident">metrics</span>.<span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="ident">y_pred_batch</span>, <span class="kw-2">&amp;</span><span class="ident">y_true_batch</span>);
                        <span class="ident">metrics_values</span>[<span class="ident">i</span>] <span class="op">+</span><span class="op">=</span> <span class="ident">metrics_value</span>;
                    }
                }
                <span class="comment">// Divide by number of batches</span>
                <span class="kw">for</span> <span class="ident">metric</span> <span class="kw">in</span> <span class="ident">metrics_values</span>.<span class="ident">iter_mut</span>() {
                    <span class="kw-2">*</span><span class="ident">metric</span> <span class="op">/</span><span class="op">=</span> <span class="ident">num_batches</span>;
                }
            },
            <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> {},
        }
        <span class="ident">metrics_values</span>
    }


    <span class="doccomment">/// Updates the parameters of the model.</span>
    <span class="kw">fn</span> <span class="ident">update_parameters</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="self">self</span>) {
        <span class="self">self</span>.<span class="ident">optimizer</span>.<span class="ident">update_time_step</span>();
        <span class="kw">for</span> (<span class="ident">idx</span>, <span class="ident">layer</span>) <span class="kw">in</span> <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">iter_mut</span>().<span class="ident">enumerate</span>() {
            <span class="self">self</span>.<span class="ident">optimizer</span>.<span class="ident">update_parameters</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="kw-2">*</span><span class="kw-2">*</span><span class="ident">layer</span>, <span class="ident">idx</span>);
        }
    }



    <span class="doccomment">/// Computes the output of the network for the given input.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `input`: tensor of inputs. Multiple samples can be evaluated at once by stacking them along the fourth dimension of the tensor.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Returns</span>
    <span class="doccomment">/// Tensor of the predicted output</span>
    <span class="doccomment">///</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">predict</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">input</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Tensor</span> {

        <span class="comment">// Normalize the input if the network has been trained on normalized samples</span>
        <span class="kw">match</span> <span class="self">self</span>.<span class="ident">data</span>.<span class="ident">x_train_stats</span>() {
            <span class="prelude-val">Some</span>(<span class="ident">stats</span>) <span class="op">=</span><span class="op">&gt;</span> {},
            <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> {}
        }

        <span class="comment">// Compute the output of the network</span>
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">y_pred</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">forward</span>(<span class="kw-2">&amp;</span><span class="ident">input</span>);

        <span class="comment">// Unscale the output values</span>
        <span class="kw">match</span> <span class="self">self</span>.<span class="ident">data</span>.<span class="ident">y_train_stats</span>() {
            <span class="prelude-val">Some</span>(<span class="ident">stats</span>) <span class="op">=</span><span class="op">&gt;</span> {
                <span class="kw">match</span> <span class="ident">stats</span>.<span class="number">0</span> {
                    <span class="ident">Scaling</span>::<span class="ident">Normalized</span> <span class="op">=</span><span class="op">&gt;</span> {
                        <span class="kw">let</span> <span class="ident">y_min</span> <span class="op">=</span> <span class="kw-2">&amp;</span><span class="ident">stats</span>.<span class="number">1</span>;
                        <span class="kw">let</span> <span class="ident">y_max</span> <span class="op">=</span> <span class="kw-2">&amp;</span><span class="ident">stats</span>.<span class="number">2</span>;
                        <span class="kw">let</span> <span class="ident">range</span> <span class="op">=</span> <span class="ident">y_max</span> <span class="op">-</span> <span class="ident">y_min</span>;
                        <span class="ident">y_pred</span> <span class="op">=</span> <span class="ident">add</span>(<span class="kw-2">&amp;</span><span class="ident">mul</span>(<span class="kw-2">&amp;</span><span class="ident">range</span>, <span class="kw-2">&amp;</span><span class="ident">y_pred</span>, <span class="bool-val">true</span>), <span class="ident">y_max</span>, <span class="bool-val">true</span>);
                    },
                    <span class="ident">Scaling</span>::<span class="ident">Standarized</span> <span class="op">=</span><span class="op">&gt;</span> {
                        <span class="kw">let</span> <span class="ident">mean</span> <span class="op">=</span> <span class="kw-2">&amp;</span><span class="ident">stats</span>.<span class="number">1</span>;
                        <span class="kw">let</span> <span class="ident">std</span> <span class="op">=</span> <span class="kw-2">&amp;</span><span class="ident">stats</span>.<span class="number">2</span>;
                        <span class="ident">y_pred</span> <span class="op">=</span> <span class="ident">add</span>(<span class="kw-2">&amp;</span><span class="ident">mul</span>(<span class="ident">std</span>, <span class="kw-2">&amp;</span><span class="ident">y_pred</span>, <span class="bool-val">true</span>), <span class="ident">mean</span>, <span class="bool-val">true</span>);
                    }
                }
            },
            <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> {},
        }
        <span class="ident">y_pred</span>
    }

    <span class="doccomment">/// Predicts the class for the input.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `input`: tensor of inputs. Multiple samples can be evaluated at once by stacking them along the fourth dimension of the tensor.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Returns</span>
    <span class="doccomment">/// Vector of tuples containing the predicted class and the probability for each sample.</span>
    <span class="doccomment">///</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">predict_class</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">input</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Vec</span><span class="op">&lt;</span>(<span class="ident">String</span>, <span class="ident">PrimitiveType</span>)<span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">input</span>.<span class="ident">dims</span>().<span class="ident">get</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">usize</span>;
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">predictions</span>: <span class="ident">Vec</span><span class="op">&lt;</span>(<span class="ident">String</span>, <span class="ident">PrimitiveType</span>)<span class="op">&gt;</span> <span class="op">=</span> <span class="ident">Vec</span>::<span class="ident">with_capacity</span>(<span class="ident">batch_size</span>);

        <span class="comment">// Compute the output of the network and retrieve value and index of maximum value</span>
        <span class="kw">let</span> <span class="ident">y_pred</span> <span class="op">=</span> <span class="self">self</span>.<span class="ident">predict</span>(<span class="ident">input</span>);
        <span class="kw">let</span> (<span class="ident">probabilities_tensor</span>, <span class="ident">class_idxs_tensor</span>) <span class="op">=</span> <span class="ident">imax</span>(<span class="kw-2">&amp;</span><span class="ident">y_pred</span>, <span class="number">0</span>);

        <span class="comment">// Retrieve values from GPU</span>
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">probabilities</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">PrimitiveType</span><span class="op">&gt;</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="number">0</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>; <span class="ident">batch_size</span> <span class="kw">as</span> <span class="ident">usize</span>];
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">class_idxs</span>: <span class="ident">Vec</span><span class="op">&lt;</span><span class="ident">u32</span><span class="op">&gt;</span> <span class="op">=</span> <span class="macro">vec</span><span class="macro">!</span>[<span class="number">0</span>; <span class="ident">batch_size</span> <span class="kw">as</span> <span class="ident">usize</span>];
        <span class="ident">probabilities_tensor</span>.<span class="ident">host</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">probabilities</span>);
        <span class="ident">class_idxs_tensor</span>.<span class="ident">host</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">class_idxs</span>);

        <span class="comment">// Build output structure</span>
        <span class="kw">match</span> <span class="kw-2">&amp;</span><span class="self">self</span>.<span class="ident">classes</span> {
            <span class="prelude-val">Some</span>(<span class="ident">classes</span>) <span class="op">=</span><span class="op">&gt;</span> {
                <span class="kw">for</span> <span class="ident">i</span> <span class="kw">in</span> <span class="number">0</span>..<span class="ident">batch_size</span> {
                    <span class="comment">// Handle multiclass and binary classification</span>
                    <span class="kw">if</span> <span class="ident">classes</span>.<span class="ident">len</span>() <span class="op">&gt;</span> <span class="number">2</span> {
                        <span class="ident">predictions</span>.<span class="ident">push</span>((<span class="ident">classes</span>[<span class="ident">class_idxs</span>[<span class="ident">i</span>] <span class="kw">as</span> <span class="ident">usize</span>].<span class="ident">clone</span>(), <span class="ident">probabilities</span>[<span class="ident">i</span>]));
                    } <span class="kw">else</span> {
                        <span class="kw">let</span> <span class="ident">idx</span> <span class="op">=</span> <span class="ident">probabilities</span>[<span class="ident">i</span>].<span class="ident">round</span>() <span class="kw">as</span> <span class="ident">usize</span>;
                        <span class="kw">if</span> <span class="ident">probabilities</span>[<span class="ident">i</span>] <span class="op">&lt;</span> <span class="number">0.5</span> { <span class="ident">probabilities</span>[<span class="ident">i</span>] <span class="op">=</span> <span class="number">1.</span> <span class="op">-</span> <span class="ident">probabilities</span>[<span class="ident">i</span>]; }
                        <span class="ident">predictions</span>.<span class="ident">push</span>((<span class="ident">classes</span>[<span class="ident">idx</span>].<span class="ident">clone</span>(), <span class="ident">probabilities</span>[<span class="ident">i</span>]));
                    }
                }
            },
            <span class="prelude-val">None</span> <span class="op">=</span><span class="op">&gt;</span> <span class="macro">panic</span><span class="macro">!</span>(<span class="string">&quot;The network is not aware of any classes.&quot;</span>),
        }
        <span class="ident">predictions</span>
    }


    <span class="doccomment">/// Saves the model.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `filename`: name of the file where the model is saved</span>
    <span class="doccomment">///</span>
    <span class="kw">pub</span> <span class="kw">fn</span> <span class="ident">save</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">filename</span>: <span class="kw-2">&amp;</span><span class="ident">str</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">hdf5</span>::<span class="prelude-ty">Result</span><span class="op">&lt;</span>()<span class="op">&gt;</span> {
        <span class="kw">let</span> <span class="ident">file</span> <span class="op">=</span> <span class="ident">hdf5</span>::<span class="ident">File</span>::<span class="ident">open</span>(<span class="ident">filename</span>, <span class="string">&quot;w&quot;</span>)<span class="question-mark">?</span>;


        <span class="comment">/*
        let f = fs::File::create(filename)?;
        {
            let mut writer = BufWriter::new(f);
            writer.write(&amp;self.loss_function.id().to_be_bytes())?;
            writer.write(b&quot;\n&quot;)?;
            self.optimizer.save(&amp;mut writer)?;

            // Save layers
            for layer in self.layers.iter() {
                layer.save(&amp;mut writer)?;
            }
        }
        */</span>
        <span class="prelude-val">Ok</span>(())
    }

}


<span class="kw">impl</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="ident">D</span>, <span class="ident">O</span>, <span class="ident">L</span><span class="op">&gt;</span> <span class="ident">fmt</span>::<span class="ident">Display</span> <span class="kw">for</span> <span class="ident">Network</span><span class="op">&lt;</span><span class="lifetime">&#39;a</span>, <span class="ident">D</span>, <span class="ident">O</span>, <span class="ident">L</span><span class="op">&gt;</span>
<span class="kw">where</span> <span class="ident">D</span>: <span class="ident">DataSet</span>, <span class="ident">O</span>: <span class="ident">Optimizer</span>, <span class="ident">L</span>: <span class="ident">Loss</span>
{
    <span class="kw">fn</span> <span class="ident">fmt</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">f</span>: <span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">fmt</span>::<span class="ident">Formatter</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">fmt</span>::<span class="prelude-ty">Result</span> {
        <span class="macro">writeln</span><span class="macro">!</span>(<span class="ident">f</span>, <span class="string">&quot;Layer \t\t Parameters&quot;</span>)<span class="question-mark">?</span>;
        <span class="macro">writeln</span><span class="macro">!</span>(<span class="ident">f</span>, <span class="string">&quot;------------------------&quot;</span>)<span class="question-mark">?</span>;
        <span class="kw">for</span> <span class="ident">layer</span> <span class="kw">in</span> <span class="self">self</span>.<span class="ident">layers</span>.<span class="ident">iter</span>() {
            <span class="macro">println</span><span class="macro">!</span>(<span class="string">&quot;{}&quot;</span>, <span class="ident">layer</span>);
        }
        <span class="prelude-val">Ok</span>(())
    }
}</pre></div>
</section><section id="search" class="content hidden"></section><section class="footer"></section><aside id="help" class="hidden"><div><h1 class="hidden">Help</h1><div class="shortcuts"><h2>Keyboard Shortcuts</h2><dl><dt><kbd>?</kbd></dt><dd>Show this help dialog</dd><dt><kbd>S</kbd></dt><dd>Focus the search field</dd><dt><kbd>↑</kbd></dt><dd>Move up in search results</dd><dt><kbd>↓</kbd></dt><dd>Move down in search results</dd><dt><kbd>↹</kbd></dt><dd>Switch tab</dd><dt><kbd>&#9166;</kbd></dt><dd>Go to active search result</dd><dt><kbd>+</kbd></dt><dd>Expand all sections</dd><dt><kbd>-</kbd></dt><dd>Collapse all sections</dd></dl></div><div class="infos"><h2>Search Tricks</h2><p>Prefix searches with a type followed by a colon (e.g., <code>fn:</code>) to restrict the search to a given type.</p><p>Accepted types are: <code>fn</code>, <code>mod</code>, <code>struct</code>, <code>enum</code>, <code>trait</code>, <code>type</code>, <code>macro</code>, and <code>const</code>.</p><p>Search functions by type signature (e.g., <code>vec -> usize</code> or <code>* -> vec</code>)</p><p>Search multiple things at once by splitting your query with comma (e.g., <code>str,u8</code> or <code>String,struct:Vec,test</code>)</p></div></div></aside><script>window.rootPath = "../../";window.currentCrate = "neuro";</script><script src="../../aliases.js"></script><script src="../../main.js"></script><script src="../../source-script.js"></script><script src="../../source-files.js"></script><script defer src="../../search-index.js"></script></body></html>