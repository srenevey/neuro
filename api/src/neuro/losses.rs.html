<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source to the Rust file `src/losses.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>losses.rs.html -- source</title><link rel="stylesheet" type="text/css" href="../../normalize.css"><link rel="stylesheet" type="text/css" href="../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" type="text/css" href="../../dark.css"><link rel="stylesheet" type="text/css" href="../../light.css" id="themeStyle"><script src="../../storage.js"></script><noscript><link rel="stylesheet" href="../../noscript.css"></noscript><link rel="shortcut icon" href="../../favicon.ico"><style type="text/css">#crate-search{background-image:url("../../down-arrow.svg");}</style></head><body class="rustdoc source"><!--[if lte IE 8]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="sidebar"><div class="sidebar-menu">&#9776;</div><a href='../../neuro/index.html'><div class='logo-container'><img src='../../rust-logo.png' alt='logo'></div></a></nav><div class="theme-picker"><button id="theme-picker" aria-label="Pick another theme!"><img src="../../brush.svg" width="18" alt="Pick another theme!"></button><div id="theme-choices"></div></div><script src="../../theme.js"></script><nav class="sub"><form class="search-form js-only"><div class="search-container"><div><select id="crate-search"><option value="All crates">All crates</option></select><input class="search-input" name="search" autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"></div><a id="settings-menu" href="../../settings.html"><img src="../../wheel.svg" width="18" alt="Change settings"></a></div></form></nav><section id="main" class="content"><pre class="line-numbers"><span id="1">  1</span>
<span id="2">  2</span>
<span id="3">  3</span>
<span id="4">  4</span>
<span id="5">  5</span>
<span id="6">  6</span>
<span id="7">  7</span>
<span id="8">  8</span>
<span id="9">  9</span>
<span id="10"> 10</span>
<span id="11"> 11</span>
<span id="12"> 12</span>
<span id="13"> 13</span>
<span id="14"> 14</span>
<span id="15"> 15</span>
<span id="16"> 16</span>
<span id="17"> 17</span>
<span id="18"> 18</span>
<span id="19"> 19</span>
<span id="20"> 20</span>
<span id="21"> 21</span>
<span id="22"> 22</span>
<span id="23"> 23</span>
<span id="24"> 24</span>
<span id="25"> 25</span>
<span id="26"> 26</span>
<span id="27"> 27</span>
<span id="28"> 28</span>
<span id="29"> 29</span>
<span id="30"> 30</span>
<span id="31"> 31</span>
<span id="32"> 32</span>
<span id="33"> 33</span>
<span id="34"> 34</span>
<span id="35"> 35</span>
<span id="36"> 36</span>
<span id="37"> 37</span>
<span id="38"> 38</span>
<span id="39"> 39</span>
<span id="40"> 40</span>
<span id="41"> 41</span>
<span id="42"> 42</span>
<span id="43"> 43</span>
<span id="44"> 44</span>
<span id="45"> 45</span>
<span id="46"> 46</span>
<span id="47"> 47</span>
<span id="48"> 48</span>
<span id="49"> 49</span>
<span id="50"> 50</span>
<span id="51"> 51</span>
<span id="52"> 52</span>
<span id="53"> 53</span>
<span id="54"> 54</span>
<span id="55"> 55</span>
<span id="56"> 56</span>
<span id="57"> 57</span>
<span id="58"> 58</span>
<span id="59"> 59</span>
<span id="60"> 60</span>
<span id="61"> 61</span>
<span id="62"> 62</span>
<span id="63"> 63</span>
<span id="64"> 64</span>
<span id="65"> 65</span>
<span id="66"> 66</span>
<span id="67"> 67</span>
<span id="68"> 68</span>
<span id="69"> 69</span>
<span id="70"> 70</span>
<span id="71"> 71</span>
<span id="72"> 72</span>
<span id="73"> 73</span>
<span id="74"> 74</span>
<span id="75"> 75</span>
<span id="76"> 76</span>
<span id="77"> 77</span>
<span id="78"> 78</span>
<span id="79"> 79</span>
<span id="80"> 80</span>
<span id="81"> 81</span>
<span id="82"> 82</span>
<span id="83"> 83</span>
<span id="84"> 84</span>
<span id="85"> 85</span>
<span id="86"> 86</span>
<span id="87"> 87</span>
<span id="88"> 88</span>
<span id="89"> 89</span>
<span id="90"> 90</span>
<span id="91"> 91</span>
<span id="92"> 92</span>
<span id="93"> 93</span>
<span id="94"> 94</span>
<span id="95"> 95</span>
<span id="96"> 96</span>
<span id="97"> 97</span>
<span id="98"> 98</span>
<span id="99"> 99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
</pre><div class="example-wrap"><pre class="rust ">
<span class="doccomment">//! Loss functions.</span>
<span class="kw">use</span> <span class="ident">arrayfire</span>::<span class="kw-2">*</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">Tensor</span>;
<span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">tensor</span>::<span class="kw-2">*</span>;

<span class="doccomment">/// Defines the behaviors of a loss function.</span>
<span class="kw">pub</span> <span class="kw">trait</span> <span class="ident">Loss</span> {
    <span class="doccomment">/// Returns a unique identifier.</span>
    <span class="kw">fn</span> <span class="ident">id</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">u64</span>;

    <span class="doccomment">/// Computes the value of the loss function.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `y_pred`: predicted labels</span>
    <span class="doccomment">/// * `y_true`: true labels</span>
    <span class="doccomment">///</span>
    <span class="kw">fn</span> <span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>, <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">PrimitiveType</span>;

    <span class="doccomment">/// Computes the gradient of the loss function.</span>
    <span class="doccomment">///</span>
    <span class="doccomment">/// # Arguments</span>
    <span class="doccomment">/// * `y_pred`: predicted labels</span>
    <span class="doccomment">/// * `y_true`: true labels</span>
    <span class="doccomment">///</span>
    <span class="kw">fn</span> <span class="ident">grad</span>(<span class="kw-2">&amp;</span><span class="self">self</span>, <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>, <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Tensor</span>;
}

<span class="doccomment">/// Defines the binary cross entropy loss.</span>
<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Copy</span>, <span class="ident">Clone</span>)]</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">BinaryCrossEntropy</span>;

<span class="kw">impl</span> <span class="ident">Loss</span> <span class="kw">for</span> <span class="ident">BinaryCrossEntropy</span> {
    <span class="kw">fn</span> <span class="ident">id</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">u64</span> {
        <span class="number">1</span>
    }

    <span class="kw">fn</span> <span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">PrimitiveType</span> {
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="comment">// Prevent the log to explode by clipping the predicted values</span>
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">loss</span> <span class="op">=</span> <span class="ident">clamp</span>(<span class="ident">y_pred</span>, <span class="kw-2">&amp;</span>(<span class="number">1e-8</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>), <span class="kw-2">&amp;</span>((<span class="number">1.</span> <span class="op">-</span> <span class="number">1e-8</span>) <span class="kw">as</span> <span class="ident">PrimitiveType</span>), <span class="bool-val">true</span>);
        <span class="ident">loss</span> <span class="op">=</span> <span class="ident">y_true</span> <span class="op">*</span> <span class="ident">log</span>(<span class="kw-2">&amp;</span><span class="ident">loss</span>) <span class="op">+</span> (<span class="ident">Tensor</span>::<span class="ident">ones</span>(<span class="ident">y_true</span>.<span class="ident">dims</span>()) <span class="op">-</span> <span class="ident">y_true</span>) <span class="op">*</span> <span class="ident">log</span>(<span class="kw-2">&amp;</span><span class="ident">sub</span>(<span class="kw-2">&amp;</span><span class="ident">Tensor</span>::<span class="ident">ones</span>(<span class="ident">loss</span>.<span class="ident">dims</span>()), <span class="kw-2">&amp;</span><span class="ident">loss</span>, <span class="bool-val">true</span>));
        <span class="op">-</span><span class="number">1.</span> <span class="op">/</span> <span class="ident">batch_size</span> <span class="op">*</span> <span class="ident">sum_all</span>(<span class="kw-2">&amp;</span><span class="ident">loss</span>).<span class="number">0</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>
        <span class="comment">//*loss = sum(&amp;sum(&amp;sum(&amp;sum(&amp;loss, 3), 2), 1), 0);</span>
        <span class="comment">//*loss = mul(loss, &amp;(-1. / batch_size), true);</span>
        <span class="comment">//-1. / batch_size * sum_all(&amp;(y_true * log(&amp;output) + (Tensor::ones(y_true.dims()) - y_true) * log(&amp;(Tensor::ones(output.dims()) - output)))).0 as PrimitiveType</span>
    }

    <span class="kw">fn</span> <span class="ident">grad</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Tensor</span> {
        <span class="kw">let</span> <span class="ident">ones</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">ones</span>(<span class="ident">y_true</span>.<span class="ident">dims</span>());
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="kw">let</span> <span class="ident">factor</span> <span class="op">=</span> <span class="number">1.</span> <span class="op">/</span> <span class="ident">batch_size</span>;
        <span class="op">-</span>  (<span class="ident">y_true</span> <span class="op">/</span> <span class="ident">y_pred</span> <span class="op">-</span> (<span class="kw-2">&amp;</span><span class="ident">ones</span> <span class="op">-</span> <span class="ident">y_true</span>) <span class="op">/</span> (<span class="kw-2">&amp;</span><span class="ident">ones</span> <span class="op">-</span> <span class="ident">y_pred</span>)) <span class="op">*</span> <span class="ident">factor</span>
    }
}

<span class="doccomment">/// Defines the cross entropy loss.</span>
<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Copy</span>, <span class="ident">Clone</span>)]</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">CrossEntropy</span>;

<span class="kw">impl</span> <span class="ident">Loss</span> <span class="kw">for</span> <span class="ident">CrossEntropy</span> {
    <span class="kw">fn</span> <span class="ident">id</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">u64</span> {
        <span class="number">2</span>
    }

    <span class="kw">fn</span> <span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">PrimitiveType</span> {
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="comment">// Prevent the log to explode by clipping the predicted values</span>
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">loss</span> <span class="op">=</span> <span class="ident">clamp</span>(<span class="ident">y_pred</span>, <span class="kw-2">&amp;</span>(<span class="number">1e-8</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>), <span class="kw-2">&amp;</span>((<span class="number">1.</span> <span class="op">-</span> <span class="number">1e-8</span>) <span class="kw">as</span> <span class="ident">PrimitiveType</span>), <span class="bool-val">true</span>);
        <span class="ident">loss</span> <span class="op">=</span> <span class="ident">mul</span>(<span class="ident">y_true</span>, <span class="kw-2">&amp;</span><span class="ident">log</span>(<span class="kw-2">&amp;</span><span class="ident">loss</span>), <span class="bool-val">true</span>);
        <span class="op">-</span><span class="number">1.</span> <span class="op">/</span> <span class="ident">batch_size</span> <span class="op">*</span> <span class="ident">sum_all</span>(<span class="kw-2">&amp;</span><span class="ident">loss</span>).<span class="number">0</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>
        <span class="comment">//*loss = sum(&amp;sum(&amp;sum(&amp;sum(loss, 3), 2), 1), 0);</span>
        <span class="comment">//*loss = mul(loss, &amp;(-1. / batch_size), true);</span>
        <span class="comment">//- 1. / batch_size * sum_all(&amp;mul(y_true, &amp;log(&amp;output), true)).0 as PrimitiveType</span>
    }

    <span class="kw">fn</span> <span class="ident">grad</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Tensor</span> {
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="kw">let</span> <span class="ident">factor</span> <span class="op">=</span> <span class="number">1.</span> <span class="op">/</span> <span class="ident">batch_size</span>;
        <span class="op">-</span> (<span class="ident">y_true</span> <span class="op">/</span> <span class="ident">y_pred</span>) <span class="op">*</span> <span class="ident">factor</span>
    }
}

<span class="doccomment">/// Defines the mean absolute error loss (MAE).</span>
<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Copy</span>, <span class="ident">Clone</span>)]</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">MeanAbsoluteError</span>;

<span class="kw">impl</span> <span class="ident">Loss</span> <span class="kw">for</span> <span class="ident">MeanAbsoluteError</span> {
    <span class="kw">fn</span> <span class="ident">id</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">u64</span> {
        <span class="number">3</span>
    }

    <span class="kw">fn</span> <span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">PrimitiveType</span> {
        <span class="comment">//*loss = abs(&amp;sub(y_pred, y_true, true));</span>
        <span class="comment">//*loss = mean(&amp;mean(&amp;mean(&amp;mean(loss, 3), 2), 1), 0);</span>
        <span class="ident">mean_all</span>(<span class="kw-2">&amp;</span><span class="ident">abs</span>(<span class="kw-2">&amp;</span><span class="ident">sub</span>(<span class="ident">y_pred</span>, <span class="ident">y_true</span>, <span class="bool-val">true</span>))).<span class="number">0</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>
    }

    <span class="kw">fn</span> <span class="ident">grad</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Tensor</span> {
        <span class="kw">let</span> <span class="ident">diff</span> <span class="op">=</span> <span class="ident">y_pred</span> <span class="op">-</span> <span class="ident">y_true</span>;
        <span class="kw">let</span> <span class="ident">cond</span> <span class="op">=</span> <span class="ident">ge</span>(<span class="ident">y_pred</span>, <span class="ident">y_true</span>, <span class="bool-val">true</span>);
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="kw">let</span> <span class="ident">factor</span> <span class="op">=</span> <span class="number">1.</span> <span class="op">/</span> <span class="ident">batch_size</span>;
        <span class="ident">mul</span>(<span class="kw-2">&amp;</span><span class="ident">constant</span>(<span class="ident">factor</span>, <span class="ident">y_pred</span>.<span class="ident">dims</span>()), <span class="kw-2">&amp;</span><span class="ident">selectr</span>(<span class="kw-2">&amp;</span><span class="ident">Tensor</span>::<span class="ident">ones</span>(<span class="ident">y_pred</span>.<span class="ident">dims</span>()), <span class="kw-2">&amp;</span><span class="ident">cond</span>, <span class="op">-</span><span class="number">1.0f64</span>), <span class="bool-val">true</span>)
    }
}

<span class="doccomment">/// Defines the mean squared error loss (MSE).</span>
<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Copy</span>, <span class="ident">Clone</span>)]</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">MeanSquaredError</span>;

<span class="kw">impl</span> <span class="ident">Loss</span> <span class="kw">for</span> <span class="ident">MeanSquaredError</span> {
    <span class="kw">fn</span> <span class="ident">id</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">u64</span> {
        <span class="number">4</span>
    }

    <span class="kw">fn</span> <span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">PrimitiveType</span> {
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="comment">//*loss = pow(&amp;(y_pred - y_true), &amp;(2.0 as PrimitiveType), true);</span>
        <span class="comment">//*loss = sum(&amp;sum(&amp;sum(&amp;sum(loss, 3), 2), 1), 0);</span>
        <span class="comment">//*loss = mul(loss, &amp;(1. / batch_size), true);</span>
        <span class="number">1.</span> <span class="op">/</span> <span class="ident">batch_size</span> <span class="op">*</span> <span class="ident">sum_all</span>(<span class="kw-2">&amp;</span><span class="ident">pow</span>(<span class="kw-2">&amp;</span><span class="ident">sub</span>(<span class="ident">y_pred</span>, <span class="ident">y_true</span>, <span class="bool-val">true</span>), <span class="kw-2">&amp;</span><span class="number">2.0f64</span>, <span class="bool-val">true</span>)).<span class="number">0</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>
    }

    <span class="kw">fn</span> <span class="ident">grad</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Tensor</span> {
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="kw">let</span> <span class="ident">factor</span> <span class="op">=</span> <span class="number">2.</span> <span class="op">/</span> <span class="ident">batch_size</span>;
        (<span class="ident">y_pred</span> <span class="op">-</span> <span class="ident">y_true</span>) <span class="op">*</span> <span class="ident">factor</span>
    }
}


<span class="doccomment">/// Applies the softmax function on the input and then computes the cross entropy loss.</span>
<span class="attribute">#[<span class="ident">derive</span>(<span class="ident">Debug</span>, <span class="ident">Copy</span>, <span class="ident">Clone</span>)]</span>
<span class="kw">pub</span> <span class="kw">struct</span> <span class="ident">SoftmaxCrossEntropy</span>;

<span class="kw">impl</span> <span class="ident">Loss</span> <span class="kw">for</span> <span class="ident">SoftmaxCrossEntropy</span> {
    <span class="kw">fn</span> <span class="ident">id</span>(<span class="kw-2">&amp;</span><span class="self">self</span>) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">u64</span> {
        <span class="number">5</span>
    }

    <span class="kw">fn</span> <span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">PrimitiveType</span> {
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="comment">// Prevent the log to explode by clipping the predicted values</span>
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">loss</span> <span class="op">=</span> <span class="ident">clamp</span>(<span class="ident">y_pred</span>, <span class="kw-2">&amp;</span>(<span class="number">1e-8</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>), <span class="kw-2">&amp;</span>((<span class="number">1.</span> <span class="op">-</span> <span class="number">1e-8</span>) <span class="kw">as</span> <span class="ident">PrimitiveType</span>), <span class="bool-val">true</span>);
        <span class="ident">loss</span> <span class="op">=</span> <span class="ident">y_true</span> <span class="op">*</span> <span class="kw-2">&amp;</span><span class="ident">log</span>(<span class="kw-2">&amp;</span><span class="ident">loss</span>);
        <span class="comment">//*loss = sum(&amp;sum(&amp;sum(&amp;sum(loss, 3), 2), 1), 0);</span>
        <span class="comment">//*loss = mul(loss, &amp;(- 1. / batch_size), true);</span>
        <span class="op">-</span> <span class="number">1.</span> <span class="op">/</span> <span class="ident">batch_size</span> <span class="op">*</span> <span class="ident">sum_all</span>(<span class="kw-2">&amp;</span><span class="ident">loss</span>).<span class="number">0</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>
    }

    <span class="kw">fn</span> <span class="ident">grad</span>(<span class="kw-2">&amp;</span><span class="self">self</span>,
            <span class="ident">y_pred</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>,
            <span class="ident">y_true</span>: <span class="kw-2">&amp;</span><span class="ident">Tensor</span>
    ) <span class="op">-</span><span class="op">&gt;</span> <span class="ident">Tensor</span> {
        <span class="kw">let</span> <span class="ident">batch_size</span> <span class="op">=</span> <span class="ident">y_pred</span>.<span class="ident">dims</span>()[<span class="number">3</span>] <span class="kw">as</span> <span class="ident">PrimitiveType</span>;
        <span class="kw">let</span> <span class="ident">factor</span> <span class="op">=</span> <span class="number">1.</span> <span class="op">/</span> <span class="ident">batch_size</span>;
        (<span class="ident">y_pred</span> <span class="op">-</span> <span class="ident">y_true</span>) <span class="op">*</span> <span class="ident">factor</span>
    }
}




<span class="attribute">#[<span class="ident">cfg</span>(<span class="ident">test</span>)]</span>
<span class="kw">mod</span> <span class="ident">tests</span> {
    <span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">losses</span>::<span class="kw-2">*</span>;
    <span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">Tensor</span>;
    <span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">tensor</span>::<span class="kw-2">*</span>;
    <span class="kw">use</span> <span class="kw">crate</span>::<span class="ident">assert_approx_eq</span>;

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_mse_eval</span>() {
        <span class="kw">let</span> <span class="ident">loss_fun</span> <span class="op">=</span> <span class="ident">MeanSquaredError</span>;

        <span class="comment">// 1 sample, 3 outputs</span>
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">y_true</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">2.1</span>, <span class="op">-</span><span class="number">1.5</span>, <span class="number">10.9</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]));
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">y_pred</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">2.5</span>, <span class="number">0.1</span>, <span class="number">11.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]));
        <span class="comment">//let mut loss_value1 = Tensor::new(&amp;[0.0], Dim::new(&amp;[1, 1, 1, 1]));</span>
        <span class="kw">let</span> <span class="ident">loss1</span> <span class="op">=</span> <span class="ident">loss_fun</span>.<span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="ident">y_true</span>, <span class="kw-2">&amp;</span><span class="ident">y_pred</span>);
        <span class="comment">//let loss1 = loss_value1.get_scalar();</span>
        <span class="kw">let</span> <span class="ident">expected_output1</span> <span class="op">=</span> <span class="number">2.97</span>;

        <span class="comment">// 2 samples, 2 outputs</span>
        <span class="ident">y_true</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="op">-</span><span class="number">16.8</span>, <span class="number">2.34</span>, <span class="op">-</span><span class="number">0.2</span>, <span class="number">31.7</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]));
        <span class="ident">y_pred</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="op">-</span><span class="number">16.5</span>, <span class="op">-</span><span class="number">0.9</span>, <span class="op">-</span><span class="number">3.4</span>, <span class="number">29.6</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]));
        <span class="comment">//let mut loss_value2 = Tensor::new(&amp;[0.0], Dim::new(&amp;[1, 1, 1, 1]));</span>
        <span class="kw">let</span> <span class="ident">loss2</span> <span class="op">=</span> <span class="ident">loss_fun</span>.<span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="ident">y_true</span>, <span class="kw-2">&amp;</span><span class="ident">y_pred</span>);
        <span class="comment">//let loss2 = loss_value2.get_scalar();</span>
        <span class="kw">let</span> <span class="ident">expected_output2</span> <span class="op">=</span> <span class="number">12.6188</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>;

        <span class="comment">// 3 samples, 4 outputs</span>
        <span class="ident">y_true</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">254.89</span>, <span class="number">199.9</span>, <span class="op">-</span><span class="number">4.78</span>, <span class="op">-</span><span class="number">782.12</span>, <span class="number">34.65</span>, <span class="number">12.4</span>, <span class="number">5.89</span>, <span class="op">-</span><span class="number">3.2</span>, <span class="number">78.1</span>, <span class="op">-</span><span class="number">90.5</span>, <span class="op">-</span><span class="number">220.6</span>, <span class="number">136.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]));
        <span class="ident">y_pred</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">260.2</span>, <span class="number">203.0</span>, <span class="number">12.7</span>, <span class="op">-</span><span class="number">950.2</span>, <span class="number">41.3</span>, <span class="number">0.19</span>, <span class="number">7.1</span>, <span class="op">-</span><span class="number">4.0</span>, <span class="number">81.4</span>, <span class="op">-</span><span class="number">95.3</span>, <span class="op">-</span><span class="number">231.2</span>, <span class="number">128.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]));
        <span class="comment">//let mut loss_value3 = Tensor::new(&amp;[0.0], Dim::new(&amp;[1, 1, 1, 1]));</span>
        <span class="kw">let</span> <span class="ident">loss3</span> <span class="op">=</span> <span class="ident">loss_fun</span>.<span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="ident">y_true</span>, <span class="kw-2">&amp;</span><span class="ident">y_pred</span>);
        <span class="comment">//let loss3 = loss_value3.get_scalar();</span>
        <span class="kw">let</span> <span class="ident">expected_output3</span> <span class="op">=</span> <span class="number">9666.647867</span> <span class="kw">as</span> <span class="ident">PrimitiveType</span>;

        <span class="macro">assert_approx_eq</span><span class="macro">!</span>([<span class="ident">loss1</span>, <span class="ident">loss2</span>, <span class="ident">loss3</span>], [<span class="ident">expected_output1</span>, <span class="ident">expected_output2</span>, <span class="ident">expected_output3</span>]);
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_mse_grad</span>() {
        <span class="kw">let</span> <span class="ident">loss</span> <span class="op">=</span> <span class="ident">MeanSquaredError</span>;
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">y</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">254.89</span>, <span class="number">199.9</span>, <span class="op">-</span><span class="number">4.78</span>, <span class="op">-</span><span class="number">782.12</span>, <span class="number">34.65</span>, <span class="number">12.4</span>, <span class="number">5.89</span>, <span class="op">-</span><span class="number">3.2</span>, <span class="number">78.1</span>, <span class="op">-</span><span class="number">90.5</span>, <span class="op">-</span><span class="number">220.6</span>, <span class="number">136.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]));
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">y_expected</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">260.2</span>, <span class="number">203.0</span>, <span class="number">12.7</span>, <span class="op">-</span><span class="number">950.2</span>, <span class="number">41.3</span>, <span class="number">0.19</span>, <span class="number">7.1</span>, <span class="op">-</span><span class="number">4.0</span>, <span class="number">81.4</span>, <span class="op">-</span><span class="number">95.3</span>, <span class="op">-</span><span class="number">231.2</span>, <span class="number">128.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]));
        <span class="kw">let</span> <span class="ident">grad</span> <span class="op">=</span> <span class="ident">loss</span>.<span class="ident">grad</span>(<span class="kw-2">&amp;</span><span class="ident">y</span>, <span class="kw-2">&amp;</span><span class="ident">y_expected</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">output</span>: [<span class="ident">f64</span>; <span class="number">12</span>] <span class="op">=</span> [<span class="number">0.</span>; <span class="number">12</span>];
        <span class="ident">grad</span>.<span class="ident">host</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">output</span>);
        <span class="kw">let</span> <span class="ident">expected_output</span> <span class="op">=</span> [<span class="op">-</span><span class="number">3.54000000</span>, <span class="op">-</span><span class="number">2.06666667</span>, <span class="op">-</span><span class="number">11.65333333</span>, <span class="number">112.05333333</span>, <span class="op">-</span><span class="number">4.43333333</span>, <span class="number">8.14000000</span>, <span class="op">-</span><span class="number">0.80666667</span>, <span class="number">0.53333333</span>, <span class="op">-</span><span class="number">2.20000000</span>, <span class="number">3.20000000</span>, <span class="number">7.06666667</span>, <span class="number">5.33333333</span>];
        <span class="macro">assert_approx_eq</span><span class="macro">!</span>(<span class="ident">output</span>, <span class="ident">expected_output</span>);
    }

    <span class="comment">/*
    #[test]
    fn test_cross_entropy_eval() {
        let loss = CrossEntropy;

        // 4 samples, 3 outputs
        let mut y = Tensor::new(&amp;[0.2337, 0.3056, 0.4608, 0.4079, 0.1819, 0.4102, 0.4034, 0.2517, 0.3449, 0.2227, 0.2946, 0.4828], Dim::new(&amp;[3, 1, 1, 4]));
        let mut y_expected = Tensor::new(&amp;[0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.], Dim::new(&amp;[3, 1, 1, 4]));
        let output1 = loss.eval(&amp;y, &amp;y_expected);
        let expected_output1 = 0.825470261541275;
        assert_approx_eq!([output1], [expected_output1]);

    }

    #[test]
    fn test_cross_entropy_grad() {
        let loss = CrossEntropy;
        let mut y = Tensor::new(&amp;[0.2337, 0.3056, 0.4608, 0.4079, 0.1819, 0.4102, 0.4034, 0.2517, 0.3449, 0.2227, 0.2946, 0.4828], Dim::new(&amp;[3, 1, 1, 4]));
        let mut y_expected = Tensor::new(&amp;[0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.], Dim::new(&amp;[3, 1, 1, 4]));
        let grad = loss.grad(&amp;y, &amp;y_expected);
        let mut output: [f64; 12] = [0.; 12];
        grad.host(&amp;mut output);
        let expected_output = [0.2337, 0.3056, -0.5392, 0.4079, 0.1819, -0.5898, -0.5966, 0.2517, 0.3449, 0.2227, 0.2946, -0.5172];
        assert_approx_eq!(output, expected_output);

    }
    */</span>

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_mae_eval</span>() {
        <span class="kw">let</span> <span class="ident">loss</span> <span class="op">=</span> <span class="ident">MeanAbsoluteError</span>;

        <span class="comment">// 3 samples, 4 outputs</span>
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">254.89</span>, <span class="number">199.9</span>, <span class="op">-</span><span class="number">4.78</span>, <span class="op">-</span><span class="number">782.12</span>, <span class="number">34.65</span>, <span class="number">12.4</span>, <span class="number">5.89</span>, <span class="op">-</span><span class="number">3.2</span>, <span class="number">78.1</span>, <span class="op">-</span><span class="number">90.5</span>, <span class="op">-</span><span class="number">220.6</span>, <span class="number">136.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]));
        <span class="kw">let</span> <span class="ident">y_expected</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">260.2</span>, <span class="number">203.0</span>, <span class="number">12.7</span>, <span class="op">-</span><span class="number">950.2</span>, <span class="number">41.3</span>, <span class="number">0.19</span>, <span class="number">7.1</span>, <span class="op">-</span><span class="number">4.0</span>, <span class="number">81.4</span>, <span class="op">-</span><span class="number">95.3</span>, <span class="op">-</span><span class="number">231.2</span>, <span class="number">128.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]));
        <span class="comment">//let mut loss_value = Tensor::new(&amp;[0.0], Dim::new(&amp;[1, 1, 1, 1]));</span>
        <span class="kw">let</span> <span class="ident">loss_value</span><span class="op">=</span> <span class="ident">loss</span>.<span class="ident">eval</span>(<span class="kw-2">&amp;</span><span class="ident">y</span>, <span class="kw-2">&amp;</span><span class="ident">y_expected</span>);
        <span class="comment">//let loss = loss_value.get_scalar();</span>
        <span class="kw">let</span> <span class="ident">expected_output</span> <span class="op">=</span> <span class="number">80.513333333333335</span>;
        <span class="macro">assert_approx_eq</span><span class="macro">!</span>([<span class="ident">loss_value</span>], [<span class="ident">expected_output</span>]);
    }

    <span class="attribute">#[<span class="ident">test</span>]</span>
    <span class="kw">fn</span> <span class="ident">test_mae_grad</span>() {
        <span class="kw">let</span> <span class="ident">loss</span> <span class="op">=</span> <span class="ident">MeanAbsoluteError</span>;

        <span class="comment">// 3 samples, 4 outputs</span>
        <span class="kw">let</span> <span class="ident">y</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">254.89</span>, <span class="number">199.9</span>, <span class="op">-</span><span class="number">4.78</span>, <span class="op">-</span><span class="number">782.12</span>, <span class="number">34.65</span>, <span class="number">12.4</span>, <span class="number">5.89</span>, <span class="op">-</span><span class="number">3.2</span>, <span class="number">78.1</span>, <span class="op">-</span><span class="number">90.5</span>, <span class="op">-</span><span class="number">220.6</span>, <span class="number">136.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]));
        <span class="kw">let</span> <span class="ident">y_expected</span> <span class="op">=</span> <span class="ident">Tensor</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">260.2</span>, <span class="number">203.0</span>, <span class="number">12.7</span>, <span class="op">-</span><span class="number">950.2</span>, <span class="number">41.3</span>, <span class="number">0.19</span>, <span class="number">7.1</span>, <span class="op">-</span><span class="number">4.0</span>, <span class="number">81.4</span>, <span class="op">-</span><span class="number">95.3</span>, <span class="op">-</span><span class="number">231.2</span>, <span class="number">128.4</span>], <span class="ident">Dim</span>::<span class="ident">new</span>(<span class="kw-2">&amp;</span>[<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>]));
        <span class="kw">let</span> <span class="ident">grad</span> <span class="op">=</span> <span class="ident">loss</span>.<span class="ident">grad</span>(<span class="kw-2">&amp;</span><span class="ident">y</span>, <span class="kw-2">&amp;</span><span class="ident">y_expected</span>);
        <span class="kw">let</span> <span class="kw-2">mut</span> <span class="ident">output</span>: [<span class="ident">f64</span>; <span class="number">12</span>] <span class="op">=</span> [<span class="number">0.</span>; <span class="number">12</span>];
        <span class="ident">grad</span>.<span class="ident">host</span>(<span class="kw-2">&amp;</span><span class="kw-2">mut</span> <span class="ident">output</span>);
        <span class="kw">let</span> <span class="ident">expected_output</span> <span class="op">=</span> [<span class="op">-</span><span class="number">1.</span>, <span class="op">-</span><span class="number">1.</span>, <span class="op">-</span><span class="number">1.</span>, <span class="number">1.</span>, <span class="op">-</span><span class="number">1.</span>, <span class="number">1.</span>, <span class="op">-</span><span class="number">1.</span>, <span class="number">1.</span>, <span class="op">-</span><span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>];
        <span class="macro">assert_approx_eq</span><span class="macro">!</span>(<span class="ident">output</span>, <span class="ident">expected_output</span>);
    }
}
</pre></div>
</section><section id="search" class="content hidden"></section><section class="footer"></section><aside id="help" class="hidden"><div><h1 class="hidden">Help</h1><div class="shortcuts"><h2>Keyboard Shortcuts</h2><dl><dt><kbd>?</kbd></dt><dd>Show this help dialog</dd><dt><kbd>S</kbd></dt><dd>Focus the search field</dd><dt><kbd>↑</kbd></dt><dd>Move up in search results</dd><dt><kbd>↓</kbd></dt><dd>Move down in search results</dd><dt><kbd>↹</kbd></dt><dd>Switch tab</dd><dt><kbd>&#9166;</kbd></dt><dd>Go to active search result</dd><dt><kbd>+</kbd></dt><dd>Expand all sections</dd><dt><kbd>-</kbd></dt><dd>Collapse all sections</dd></dl></div><div class="infos"><h2>Search Tricks</h2><p>Prefix searches with a type followed by a colon (e.g., <code>fn:</code>) to restrict the search to a given type.</p><p>Accepted types are: <code>fn</code>, <code>mod</code>, <code>struct</code>, <code>enum</code>, <code>trait</code>, <code>type</code>, <code>macro</code>, and <code>const</code>.</p><p>Search functions by type signature (e.g., <code>vec -> usize</code> or <code>* -> vec</code>)</p><p>Search multiple things at once by splitting your query with comma (e.g., <code>str,u8</code> or <code>String,struct:Vec,test</code>)</p></div></div></aside><script>window.rootPath = "../../";window.currentCrate = "neuro";</script><script src="../../aliases.js"></script><script src="../../main.js"></script><script src="../../source-script.js"></script><script src="../../source-files.js"></script><script defer src="../../search-index.js"></script></body></html>