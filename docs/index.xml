<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neuro â€“ Knowledge Base</title>
    <link>https://srenevey.github.com/docs/</link>
    <description>Recent content in Knowledge Base on neuro</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://srenevey.github.com/docs/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Cross Entropy</title>
      <link>https://srenevey.github.com/docs/losses/crossentropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/losses/crossentropy/</guid>
      <description>
        
        
        &lt;p&gt;The cross entropy loss function is typically used in multiclass classification problems. For a model having &lt;img src=&#34;https://latex.codecogs.com/svg.latex?C&#34; title=&#34;C&#34; /&gt; classes and if &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;\hat{Y}&amp;space;=&amp;space;\begin{bmatrix}&amp;space;\boldsymbol{\hat{y}}^{(1)}&amp;space;&amp;&amp;space;\boldsymbol{\hat{y}}^{(2)}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\boldsymbol{\hat{y}}^{(m)}&amp;space;\end{bmatrix}&#34; title=&#34;\hat{Y} = \begin{bmatrix} \boldsymbol{\hat{y}}^{(1)} &amp; \boldsymbol{\hat{y}}^{(2)} &amp; \dots &amp; \boldsymbol{\hat{y}}^{(m)} \end{bmatrix}&#34; /&gt; are the predicted labels and &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;Y&amp;space;=&amp;space;\begin{bmatrix}&amp;space;\boldsymbol{y}^{(1)}&amp;space;&amp;&amp;space;\boldsymbol{y}^{(2)}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\boldsymbol{y}^{(m)}&amp;space;\end{bmatrix}&#34; title=&#34;Y = \begin{bmatrix} \boldsymbol{y}^{(1)} &amp; \boldsymbol{y}^{(2)} &amp; \dots &amp; \boldsymbol{y}^{(m)} \end{bmatrix}&#34; /&gt; are the true labels, then the cross entropy is defined as&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\mathcal{L}(\hat{Y},&amp;space;Y)&amp;space;=&amp;space;\frac{1}{m}\sum_{i=1}^m\left(-\sum_{j=1}^Cy^{(i)}_j\log\hat{y}^{(i)}_j&amp;space;\right)&#34; title=&#34;\mathcal{L}(\hat{Y}, Y) = \frac{1}{m}\sum_{i=1}^m\left(-\sum_{j=1}^Cy^{(i)}_j\log\hat{y}^{(i)}_j \right)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?m&#34; title=&#34;m&#34; /&gt; is the number of samples in the mini-batch. If we take the derivative of this function with respect to output &lt;img src=&#34;https://latex.codecogs.com/svg.latex?j&#34; title=&#34;j&#34; /&gt; of sample &lt;img src=&#34;https://latex.codecogs.com/svg.latex?i&#34; title=&#34;i&#34; /&gt; we get&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;\hat{y}_j^{(i)}}&amp;space;=&amp;space;-\frac{1}{m}\frac{y_j^{(i)}}{\hat{y}_j^{(i)}}&#34; title=&#34;\frac{\partial \mathcal{L}}{\partial \hat{y}_j^{(i)}} = -\frac{1}{m}\frac{y_j^{(i)}}{\hat{y}_j^{(i)}}&#34; /&gt;
or in vector form&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\nabla_{\hat{Y}}\mathcal{L}(\hat{Y},Y)&amp;space;=&amp;space;-\frac{1}{m}\frac{Y}{\hat{Y&amp;space;}}&#34; title=&#34;\nabla_{\hat{Y}}\mathcal{L}(\hat{Y},Y) = -\frac{1}{m}\frac{Y}{\hat{Y }}&#34; /&gt;
Now, if we assume that the loss activation of the last layer is a softmax function, we have&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\hat{y}_j^{(i)}&amp;space;=&amp;space;\frac{e^{x_j^{(i)}}}{\sum_{k=1}^ne^{x_k^{(i)}}}&#34; title=&#34;\hat{y}_j^{(i)} = \frac{e^{x_j^{(i)}}}{\sum_{k=1}^ne^{x_k^{(i)}}}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and, as seen in the derivation of the &lt;a href=&#34;https://srenevey.github.com/docs/activations/softmax/&#34; target=&#34;_blank&#34;&gt;softmax&lt;/a&gt;, the derivative is&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\hat{y}_j^{(i)}}{\partial&amp;space;x_l^{(i)}}&amp;space;=&amp;space;\hat{y}_j^{(i)}(\delta_{jl}&amp;space;-&amp;space;\hat{y}_l^{(i)})&#34; title=&#34;\frac{\partial \hat{y}_j^{(i)}}{\partial x_l^{(i)}} = \hat{y}_j^{(i)}(\delta_{jl} - \hat{y}_l^{(i)})&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hence, if we compute the derivative of &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\mathcal{L}&#34; title=&#34;\mathcal{L}&#34; /&gt; with respect to &lt;img src=&#34;https://latex.codecogs.com/svg.latex?x_l^{(i)}&#34; title=&#34;x_l^{(i)}&#34; /&gt;, we have&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;x_l^{(i)}}&amp;space;=&amp;space;\sum_{k=1}^c\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;\hat{y}_k^{(i)}}\frac{\partial&amp;space;\hat{y}_k^{(i)}}{\partial&amp;space;x_l^{(i)}}&#34; title=&#34;\frac{\partial \mathcal{L}}{\partial x_l^{(i)}} = \sum_{k=1}^c\frac{\partial \mathcal{L}}{\partial \hat{y}_k^{(i)}}\frac{\partial \hat{y}_k^{(i)}}{\partial x_l^{(i)}}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?C&#34; title=&#34;C&#34; /&gt; is the number of classes. If we replace the two fractions on the right hand side by the equations found previously, we have&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;x_l^{(i)}}&amp;space;=&amp;space;-\sum_{k=1}^C\frac{y_k^{(i)}}{\hat{y}_k^{(i)}}\hat{y}_k^{(i)}(\delta_{kl}&amp;space;-&amp;space;\hat{y}_l^{(i)})&amp;space;=&amp;space;-\sum_{k=1}^Cy_k^{(i)}(\delta_{kl}&amp;space;-&amp;space;\hat{y}_l^{(i)})&#34; title=&#34;\frac{\partial \mathcal{L}}{\partial x_l^{(i)}} = -\sum_{k=1}^C\frac{y_k^{(i)}}{\hat{y}_k^{(i)}}\hat{y}_k^{(i)}(\delta_{kl} - \hat{y}_l^{(i)}) = -\sum_{k=1}^Cy_k^{(i)}(\delta_{kl} - \hat{y}_l^{(i)})&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Separating the cases &lt;img src=&#34;https://latex.codecogs.com/svg.latex?l=k&#34; title=&#34;l=k&#34; /&gt; and &lt;img src=&#34;https://latex.codecogs.com/svg.latex?l&amp;space;\neq&amp;space;k&#34; title=&#34;l \neq k&#34; /&gt; yields
&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;x_l^{(i)}}&amp;space;=&amp;space;-y_l^{(i)}(1&amp;space;-&amp;space;\hat{y}_l^{(i)})&amp;space;&amp;plus;&amp;space;\sum_{k\neq&amp;space;l}^Cy_k^{(i)}\hat{y}_l^{(i)}&amp;space;=&amp;space;\hat{y}_l^{(i)}\left(y_l^{(i)}&amp;space;&amp;plus;&amp;space;\sum_{k\neq&amp;space;l}^Cy_k^{(i)}&amp;space;\right&amp;space;)&amp;space;-&amp;space;y_l^{(i)}&#34; title=&#34;\frac{\partial \mathcal{L}}{\partial x_l^{(i)}} = -y_l^{(i)}(1 - \hat{y}_l^{(i)}) + \sum_{k\neq l}^Cy_k^{(i)}\hat{y}_l^{(i)} = \hat{y}_l^{(i)}\left(y_l^{(i)} + \sum_{k\neq l}^Cy_k^{(i)} \right ) - y_l^{(i)}&#34; /&gt;
Since the true label are one-hot encoded, we have
&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?y_l^{(i)}&amp;space;&amp;plus;&amp;space;\sum_{k\neq&amp;space;l}^Cy_k^{(i)}&amp;space;=&amp;space;1&#34; title=&#34;y_l^{(i)} + \sum_{k\neq l}^Cy_k^{(i)} = 1&#34; /&gt;
so that&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;x_l^{(i)}}&amp;space;=&amp;space;\hat{y}_l^{(i)}&amp;space;-&amp;space;y_l^{(i)}&#34; title=&#34;\frac{\partial \mathcal{L}}{\partial x_l^{(i)}} = \hat{y}_l^{(i)} - y_l^{(i)}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We see therefore that if the activation of the last layer is a softmax function, the gradient of the cross-entropy loss with respect to the linear activation of the last layer takes the simple form&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\nabla_X\mathcal{L}(\hat{Y},&amp;space;Y)&amp;space;=&amp;space;\frac{1}{m}(\hat{Y}&amp;space;-&amp;space;Y)&#34; title=&#34;\nabla_X\mathcal{L}(\hat{Y}, Y) = \frac{1}{m}(\hat{Y} - Y)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;which can be easily computed. For this reason, the current implementation of the cross-entropy loss function in neuro assumes that the last activation is a softmax function. Conversly, the softmax activation function can only be used for the last layer of the network with the cross-entropy loss function. This might change in the future.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Mean Absolute Error</title>
      <link>https://srenevey.github.com/docs/losses/mae/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/losses/mae/</guid>
      <description>
        
        
        &lt;p&gt;The mean absolute error loss function computes the mean of the absolute values of the error. Assuming that the predicted values are &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;\hat{Y}&amp;space;=&amp;space;\begin{bmatrix}&amp;space;\boldsymbol{\hat{y}}^{(1)}&amp;space;&amp;&amp;space;\boldsymbol{\hat{y}}^{(2)}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\boldsymbol{\hat{y}}^{(m)}&amp;space;\end{bmatrix}&#34; title=&#34;\hat{Y} = \begin{bmatrix} \boldsymbol{\hat{y}}^{(1)} &amp; \boldsymbol{\hat{y}}^{(2)} &amp; \dots &amp; \boldsymbol{\hat{y}}^{(m)} \end{bmatrix}&#34; /&gt; and that the true values are &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;Y&amp;space;=&amp;space;\begin{bmatrix}&amp;space;\boldsymbol{y}^{(1)}&amp;space;&amp;&amp;space;\boldsymbol{y}^{(2)}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\boldsymbol{y}^{(m)}&amp;space;\end{bmatrix}&#34; title=&#34;Y = \begin{bmatrix} \boldsymbol{y}^{(1)} &amp; \boldsymbol{y}^{(2)} &amp; \dots &amp; \boldsymbol{y}^{(m)} \end{bmatrix}&#34; /&gt;, then the mean absolute error is given by&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\mathcal{L}(\hat{Y},Y)=\frac{1}{m}\sum_{i=1}^m|\boldsymbol{\hat{y}}^{(i)}-\boldsymbol{y}^{(i)}|&#34; title=&#34;\mathcal{L}(\hat{Y},Y)=\frac{1}{m}\sum_{i=1}^m|\boldsymbol{\hat{y}}^{(i)}-\boldsymbol{y}^{(i)}|&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?m&#34; title=&#34;m&#34; /&gt; is the number of samples in the mini-batch. Since the derivative of the absolute value is given by&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{d|x|}{dx}&amp;space;=&amp;space;\frac{x}{|x|}&amp;space;=&amp;space;\begin{cases}&amp;space;1&amp;space;&amp;\text{if&amp;space;}&amp;space;x&amp;space;&gt;&amp;space;0&amp;space;\&amp;space;\text{undefined}&amp;space;&amp;amp;\text{if&amp;space;}&amp;space;x&amp;space;=&amp;space;0&amp;space;\&amp;space;-1&amp;space;&amp;amp;\text{if&amp;space;}&amp;space;x&amp;space;&amp;lt;&amp;space;0&amp;space;\end{cases}&amp;rdquo; title=&amp;rdquo;\frac{d|x|}{dx} = \frac{x}{|x|} = \begin{cases} 1 &amp;amp;\text{if } x &amp;gt; 0 \ \text{undefined} &amp;amp;\text{if } x = 0 \ -1 &amp;amp;\text{if } x &amp;lt; 0 \end{cases}&amp;rdquo; /&amp;gt;&lt;/p&gt;

&lt;p&gt;the gradient of &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\mathcal{L}&#34; title=&#34;\mathcal{L}&#34; /&gt; with respect to &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;\hat{Y}&#34; title=&#34;\hat{Y}&#34; /&gt; yields&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\nabla_{\hat{Y}}\mathcal{L}(\hat{Y},&amp;space;Y)=\begin{cases}\frac{1}{m}&amp;space;&amp;\text{if&amp;space;}&amp;space;\boldsymbol{\hat{y}}^{(i)}&amp;space;\ge&amp;space;\boldsymbol{y}^{(i)}&amp;space;\\&amp;space;-\frac{1}{m}&amp;space;&amp;\text{if&amp;space;}&amp;space;\boldsymbol{\hat{y}}^{(i)}&amp;space;&lt;&amp;space;\boldsymbol{y}^{(i)}&amp;space;\end{cases}&#34; title=&#34;\nabla_{\hat{Y}}\mathcal{L}(\hat{Y}, Y)=\begin{cases}\frac{1}{m} &amp;\text{if } \boldsymbol{\hat{y}}^{(i)} \ge \boldsymbol{y}^{(i)} \\ -\frac{1}{m} &amp;\text{if } \boldsymbol{\hat{y}}^{(i)} &lt; \boldsymbol{y}^{(i)} \end{cases}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where the case &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\boldsymbol{\hat{y}}^{(i)}&amp;space;=&amp;space;\boldsymbol{y}^{(i)}&#34; title=&#34;\boldsymbol{\hat{y}}^{(i)} = \boldsymbol{y}^{(i)}&#34; /&gt; is arbitrarily set to &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\frac{1}{m}&#34; title=&#34;\frac{1}{m}&#34; /&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Mean Squared Error</title>
      <link>https://srenevey.github.com/docs/losses/mse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/losses/mse/</guid>
      <description>
        
        
        &lt;p&gt;The mean squared error loss function computes the mean of the square of the error of the model. If the model predicts &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;\hat{Y}&amp;space;=&amp;space;\begin{bmatrix}&amp;space;\boldsymbol{\hat{y}}^{(1)}&amp;space;&amp;&amp;space;\boldsymbol{\hat{y}}^{(2)}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\boldsymbol{\hat{y}}^{(m)}&amp;space;\end{bmatrix}&#34; title=&#34;\hat{Y} = \begin{bmatrix} \boldsymbol{\hat{y}}^{(1)} &amp; \boldsymbol{\hat{y}}^{(2)} &amp; \dots &amp; \boldsymbol{\hat{y}}^{(m)} \end{bmatrix}&#34; /&gt; and the true values are &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;Y&amp;space;=&amp;space;\begin{bmatrix}&amp;space;\boldsymbol{y}^{(1)}&amp;space;&amp;&amp;space;\boldsymbol{y}^{(2)}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\boldsymbol{y}^{(m)}&amp;space;\end{bmatrix}&#34; title=&#34;Y = \begin{bmatrix} \boldsymbol{y}^{(1)} &amp; \boldsymbol{y}^{(2)} &amp; \dots &amp; \boldsymbol{y}^{(m)} \end{bmatrix}&#34; /&gt;, then  the mean squared error is computed as&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\mathcal{L}(\hat{Y},Y)&amp;space;=&amp;space;\frac{1}{m}\sum_{i=1}^m\Vert\boldsymbol{\hat{y}}^{(i)}-\boldsymbol{y}^{(i)}\Vert^2&#34; title=&#34;\mathcal{L}(\hat{Y},Y) = \frac{1}{m}\sum_{i=1}^m\Vert\boldsymbol{\hat{y}}^{(i)}-\boldsymbol{y}^{(i)}\Vert^2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?m&#34; title=&#34;m&#34; /&gt; is the number of samples in the mini-batch. Taking the gradient of this function with respect to the predicted values yields&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\nabla_{\hat{Y}}\mathcal{L}(\hat{Y},Y)=\frac{2}{m}(\hat{Y}-Y)&#34; title=&#34;\nabla_{\hat{Y}}\mathcal{L}(\hat{Y},Y)=\frac{2}{m}(\hat{Y}-Y)&#34; /&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: BatchNorm</title>
      <link>https://srenevey.github.com/docs/layers/batchnorm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/layers/batchnorm/</guid>
      <description>
        
        
        &lt;p&gt;TBD&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Conv2D</title>
      <link>https://srenevey.github.com/docs/layers/conv2d/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/layers/conv2d/</guid>
      <description>
        
        
        &lt;p&gt;TBD&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Dense</title>
      <link>https://srenevey.github.com/docs/layers/dense/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/layers/dense/</guid>
      <description>
        
        
        

&lt;p&gt;The dense, or fully connected, layer is one of the basic building blocks of neural networks. It is defined by the number of units and is&lt;/p&gt;

&lt;h2 id=&#34;forward-pass&#34;&gt;Forward Pass&lt;/h2&gt;

&lt;p&gt;During the forward pass, the inputs &lt;img src=&#34;https://latex.codecogs.com/svg.latex?o_{l-1}&#34; title=&#34;o_{l-1}&#34; /&gt;, which correspond to the output of the previous layer for a hidden layer or to the training values for the input layer, are multiplied by the weights &lt;img src=&#34;https://latex.codecogs.com/svg.latex?W^{[l]}&#34; title=&#34;W^{[l]}&#34; /&gt; of the layer and the biases &lt;img src=&#34;https://latex.codecogs.com/svg.latex?b^{[l]}&#34; title=&#34;b^{[l]}&#34; /&gt; are added to form the linear activation:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?z^{[l]}&amp;space;=&amp;space;W^{[l]}o_{l-1}&amp;plus;b^{[l]}&#34; title=&#34;z^{[l]} = W^{[l]}o_{l-1}+b^{[l]}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The linear activation is then used as input for the activation function to compute the nonlinear activation of the layer:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?o_l&amp;space;=&amp;space;g(z^{[l]})&#34; title=&#34;o_l = g(z^{[l]})&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This value is the output of the layer and is passed on to the next layer or is used to compute the loss if it is the output layer.&lt;/p&gt;

&lt;h2 id=&#34;backward-pass&#34;&gt;Backward Pass&lt;/h2&gt;

&lt;p&gt;In the backward pass, we compute how the loss function used to train the model reacts to small variations in the weights, biases, and inputs. That is, if &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\mathcal{L}&#34; title=&#34;\mathcal{L}&#34; /&gt; is the loss function, we compute &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;\nabla_{W^{[l]}}\mathcal{L}&#34; title=&#34;\nabla_{W^{[l]}}\mathcal{L}&#34; /&gt;, &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;\nabla_{b^{[l]}}\mathcal{L}&#34; title=&#34;\nabla_{b^{[l]}}\mathcal{L}&#34; /&gt;, and &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;\nabla_{o_{l-1}}\mathcal{L}&#34; title=&#34;\nabla_{o_{l-1}}\mathcal{L}&#34; /&gt;. The input of the backward pass corresponds to the gradient of the loss function with respect to the output of the layer. That is, if &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;o^{&#39;}_l&#34; title=&#34;o^{&#39;}_l&#34; /&gt; is the input of the backward pass, we have&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?o^{&#39;}_l&amp;space;\equiv&amp;space;\nabla_{o_l}\mathcal{L}&#34; title=&#34;o^{&#39;}_l \equiv \nabla_{o_l}\mathcal{L}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Applying the chain rule, we start by computing the partial derivative with respect to the weights:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;W^{[l]}}&amp;space;=&amp;space;\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;o_l}\frac{\partial&amp;space;o_l}{\partial&amp;space;W^{[l]}}&amp;space;=&amp;space;o_l^{&#39;}\frac{\partial&amp;space;o_l}{\partial&amp;space;z^{[l]}}\frac{\partial&amp;space;z^{[l]}}{\partial&amp;space;W^{[l]}}&amp;space;=&amp;space;(o_l^{&#39;}\odot&amp;space;g&#39;(z^{[l]}))o_{l-1}^T&#34; title=&#34;\frac{\partial \mathcal{L}}{\partial W^{[l]}} = \frac{\partial \mathcal{L}}{\partial o_l}\frac{\partial o_l}{\partial W^{[l]}} = o_l^{&#39;}\frac{\partial o_l}{\partial z^{[l]}}\frac{\partial z^{[l]}}{\partial W^{[l]}} = (o_l^{&#39;}\odot g&#39;(z^{[l]}))o_{l-1}^T&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\odot&#34; title=&#34;\odot&#34; /&gt; denotes the Hadamard product (i.e. element-wise product) and the transpose of &lt;img src=&#34;https://latex.codecogs.com/svg.latex?o_{l-1}&#34; title=&#34;o_{l-1}&#34; /&gt; is taken to have consistent dimensions. We then proceed with the partial derivative with respect to the biases:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;b^{[l]}}&amp;space;=&amp;space;\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;o_l}\frac{\partial&amp;space;o_l}{\partial&amp;space;b^{[l]}}&amp;space;=&amp;space;o_l^{&#39;}\frac{\partial&amp;space;o_l}{\partial&amp;space;z^{[l]}}\frac{\partial&amp;space;z^{[l]}}{\partial&amp;space;b^{[l]}}&amp;space;=&amp;space;o_l^{&#39;}\odot&amp;space;g&#39;(z^{[l]})&#34; title=&#34;\frac{\partial \mathcal{L}}{\partial b^{[l]}} = \frac{\partial \mathcal{L}}{\partial o_l}\frac{\partial o_l}{\partial b^{[l]}} = o_l^{&#39;}\frac{\partial o_l}{\partial z^{[l]}}\frac{\partial z^{[l]}}{\partial b^{[l]}} = o_l^{&#39;}\odot g&#39;(z^{[l]})&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And finally, the partial derivatives with respect to the layer&amp;rsquo;s inputs:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;o_{l-1}}&amp;space;=&amp;space;\frac{\partial&amp;space;\mathcal{L}}{\partial&amp;space;o_l}\frac{\partial&amp;space;o_l}{\partial&amp;space;o_{l-1}}&amp;space;=&amp;space;o_l^{&#39;}\frac{\partial&amp;space;o_l}{\partial&amp;space;z^{[l]}}\frac{\partial&amp;space;z^{[l]}}{\partial&amp;space;o_{l-1}}&amp;space;=&amp;space;{W^{[l]}}^T&amp;space;(o_l^{&#39;}\odot&amp;space;g&#39;(z^{[l]}))&#34; title=&#34;\frac{\partial \mathcal{L}}{\partial o_{l-1}} = \frac{\partial \mathcal{L}}{\partial o_l}\frac{\partial o_l}{\partial o_{l-1}} = o_l^{&#39;}\frac{\partial o_l}{\partial z^{[l]}}\frac{\partial z^{[l]}}{\partial o_{l-1}} = {W^{[l]}}^T (o_l^{&#39;}\odot g&#39;(z^{[l]}))&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where the transpose of the weights is taken to have consistent dimensions. This last value is the output of the backward pass and is passed on to the previous layer. The following figure illustrates the forward and backward passes of the dense layer.&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; width=&#34;320px&#34; height=&#34;396.23px&#34; src=&#34;../images/dense_layer.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The parameters of the layer are the weights and biases. During the forward pass, the parameters are used to compute the linear and nonlinear activations. The inputs and the linear activations are cached for later use by the backprop algorithm. During the backward pass, the parameters and the previously cached values are used to compute the gradients with respect to the weights and biases which are cached. These values will be used once the backprop algorithm has been computed for each layer. At that point, an optimizer will update all parameters in the network. Finally, the gradient with respect to the inputs is computed and returned.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Dropout</title>
      <link>https://srenevey.github.com/docs/layers/dropout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/layers/dropout/</guid>
      <description>
        
        
        &lt;p&gt;TBD&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Linear</title>
      <link>https://srenevey.github.com/docs/activations/linear/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/activations/linear/</guid>
      <description>
        
        
        &lt;p&gt;The linear activation function simply returns its input without applying any modification:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?g(x)&amp;space;=&amp;space;x&#34; title=&#34;g(x) = x&#34; /&gt;
&lt;img class=&#34;center&#34; src=&#34;../images/linear_activation.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Its derivative is then given by:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;g}{\partial&amp;space;x}&amp;space;=&amp;space;1&#34; title=&#34;\frac{\partial g}{\partial x} = 1&#34; /&gt;
&lt;img class=&#34;center&#34; src=&#34;../images/dlinear_activation.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This activation function is used for instance in the last layer of a regression network.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: MaxPooling2D</title>
      <link>https://srenevey.github.com/docs/layers/maxpooling2d/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/layers/maxpooling2d/</guid>
      <description>
        
        
        &lt;p&gt;TBD&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ReLU</title>
      <link>https://srenevey.github.com/docs/activations/relu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/activations/relu/</guid>
      <description>
        
        
        

&lt;p&gt;The ReLU or rectified linear unit is one of the most widely used activation functions. This function is defined as follows:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?g(x)&amp;space;=&amp;space;\max(0,&amp;space;x)&#34; title=&#34;g(x) = \max(0, x)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That is, the function returns the input value if it is positive and zero otherwise.&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;../images/relu.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Its derivative can be expressed as:
&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;g}{\partial&amp;space;x}&amp;space;=&amp;space;\begin{cases}1&amp;\text{if&amp;space;}x&gt;0\&amp;space;\text{undefined}&amp;amp;\text{if&amp;space;}x=0\&amp;space;0&amp;space;&amp;amp;\text{if&amp;space;}x&lt;0\end{cases}&#34; title=&#34;\frac{\partial g}{\partial x} = \begin{cases}1&amp;\text{if }x&gt;0\ \text{undefined}&amp;amp;\text{if }x=0\ 0 &amp;amp;\text{if }x&lt;0\end{cases}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since the case &lt;img src=&#34;https://latex.codecogs.com/svg.latex?x&amp;space;=&amp;space;0&#34; title=&#34;x = 0&#34; /&gt; is undefined, the derivative can be arbitrarily set to 1 for this value such that&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;g}{\partial&amp;space;x}&amp;space;=&amp;space;\begin{cases}&amp;space;1&amp;space;&amp;\text{if&amp;space;}&amp;space;x&amp;space;\geq&amp;space;0&amp;space;\\&amp;space;0&amp;space;&amp;\text{if&amp;space;}&amp;space;x&amp;space;&lt;&amp;space;0&amp;space;\end{cases}&#34; title=&#34;\frac{\partial g}{\partial x} = \begin{cases} 1 &amp;\text{if } x \geq 0 \\ 0 &amp;\text{if } x &lt; 0 \end{cases}&#34; /&gt;
&lt;img class=&#34;center&#34; src=&#34;../images/drelu.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The ReLU activation function has been shown to help with the vanishing gradient problem that may arise when training very deep networks [1].&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;[1] &lt;a href=&#34;http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf&#34; target=&#34;_blank&#34;&gt;Glorot X., Bordes A., Bengio Y., &amp;ldquo;Deep Sparse Rectifier Neural Networks&amp;rdquo;, Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011, Fort Lauderdale, FL, USA.&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Sigmoid</title>
      <link>https://srenevey.github.com/docs/activations/sigmoid/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/activations/sigmoid/</guid>
      <description>
        
        
        &lt;p&gt;A sigmoid function is a mathematical function that maps an input to an output following an &amp;ldquo;S&amp;rdquo;-shaped curve. The sigmoid function commonly used in deep learning is the logistic function given by&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\sigma(x)=\frac{1}{1&amp;plus;e^{-x}}&#34; title=&#34;\sigma(x)=\frac{1}{1+e^{-x}}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This function maps any input to an output in the interval (0,1).&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;../images/sigmoid.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Its derivative can be computed as&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{d\sigma}{dx}=\frac{e^{-x}}{(1&amp;plus;e^{-x})^2}&#34; title=&#34;\frac{d\sigma}{dx}=\frac{e^{-x}}{(1+e^{-x})^2}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;which can be rewritten in terms of &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\sigma&#34; title=&#34;\sigma&#34; /&gt; as&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{d\sigma}{dx}=\sigma(x)(1-\sigma(x))&#34; title=&#34;\frac{d\sigma}{dx}=\sigma(x)(1-\sigma(x))&#34; /&gt;
&lt;img class=&#34;center&#34; src=&#34;../images/dsigmoid.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This function is useful in, for instance, binary classification problems where the output of the sigmoid can be seen as the probability that the input belongs to the class.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Softmax</title>
      <link>https://srenevey.github.com/docs/activations/softmax/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/activations/softmax/</guid>
      <description>
        
        
        

&lt;p&gt;The softmax activation function is a bit different than the other activation functions in that it takes a vector as input rather than a scalar:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\vec{g}(\vec{x})=\frac{e^{\vec{x}}}{\sum_{k=1}^ne^{x_k}}&#34; title=&#34;\vec{g}(\vec{x})=\frac{e^{\vec{x}}}{\sum_{k=1}^ne^{x_k}}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\vec{x}&amp;space;=&amp;space;\begin{bmatrix}&amp;space;x_1&amp;space;&amp;&amp;space;x_2&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;x_n&amp;space;\end{bmatrix}&#34; title=&#34;\vec{x} = \begin{bmatrix} x_1 &amp; x_2 &amp; \dots &amp; x_n \end{bmatrix}&#34; /&gt;. Component-wise, we have&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?g_i(\vec{x})=\frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}&#34; title=&#34;g_i(\vec{x})=\frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the exponential terms in the above equation can quickly grow and cause numerical instability. In order to mitigate this undersired behavior, we can multiply both the numerator and denominator by a constant such that&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?g_i(\vec{x})&amp;space;=&amp;space;\frac{Ce^{x_i}}{C\sum_{k=1}^ne^{x_k}}&amp;space;=&amp;space;\frac{e^{\ln&amp;space;C}e^{x_i}}{e^{&amp;space;\ln&amp;space;C}\sum_{k=1}^ne^{x_k}}&amp;space;=&amp;space;\frac{e^{x_i&amp;space;&amp;plus;&amp;space;\ln&amp;space;C}}{\sum_{k=1}^ne^{x_k&amp;plus;\ln&amp;space;C}}&#34; title=&#34;g_i(\vec{x}) = \frac{Ce^{x_i}}{C\sum_{k=1}^ne^{x_k}} = \frac{e^{\ln C}e^{x_i}}{e^{ \ln C}\sum_{k=1}^ne^{x_k}} = \frac{e^{x_i + \ln C}}{\sum_{k=1}^ne^{x_k+\ln C}}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A common choice for the constant is to set &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\inline&amp;space;\ln&amp;space;C&amp;space;=&amp;space;-\max_k(x_k)&#34; title=&#34;\ln C = -\max_k(x_k)&#34; /&gt;  [1]. The definiton of the softmax function is such that&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\sum_{i=1}^ng_i(\vec{x})=1&#34; title=&#34;\sum_{i=1}^ng_i(\vec{x})=1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;which allows us to see &lt;img src=&#34;https://latex.codecogs.com/svg.latex?g_i(\vec{x})&#34; title=&#34;g_i(\vec{x})&#34; /&gt; as the probability that &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\vec{x}&#34; title=&#34;\vec{x}&#34; /&gt; belongs to class &lt;img src=&#34;https://latex.codecogs.com/svg.latex?i&#34; title=&#34;i&#34; /&gt;. This function is therefore widely used for the last activation of multiclass classification models.&lt;/p&gt;

&lt;p&gt;The derivative of this function is given by&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;g_i}{\partial&amp;space;x_i}&amp;space;=&amp;space;\frac{e^{x_i}\sum_{k=1}^ne^{x_k}&amp;space;-&amp;space;e^{x_i}e^{x_i}}{\left(\sum_{k=1}^ne^{x_k}&amp;space;\right)^2}&amp;space;=&amp;space;\frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}\left(1&amp;space;-&amp;space;\frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}&amp;space;\right&amp;space;)&#34; title=&#34;\frac{\partial g_i}{\partial x_i} = \frac{e^{x_i}\sum_{k=1}^ne^{x_k} - e^{x_i}e^{x_i}}{\left(\sum_{k=1}^ne^{x_k} \right)^2} = \frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}\left(1 - \frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}} \right )&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;g_i}{\partial&amp;space;x_j}&amp;space;=&amp;space;\frac{-e^{x_i}e^{x_j}}{\left(\sum_{k=1}^ne^{x_k}&amp;space;\right)^2}&amp;space;=-&amp;space;\frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}\frac{e^{x_j}}{\sum_{k=1}^ne^{x_k}}&#34; title=&#34;\frac{\partial g_i}{\partial x_j} = \frac{-e^{x_i}e^{x_j}}{\left(\sum_{k=1}^ne^{x_k} \right)^2} =- \frac{e^{x_i}}{\sum_{k=1}^ne^{x_k}}\frac{e^{x_j}}{\sum_{k=1}^ne^{x_k}}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These two equations can be rewritten more compactly as&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;g_i}{\partial&amp;space;x_j}&amp;space;=&amp;space;g_i(\vec{x})(\delta_{ij}&amp;space;-&amp;space;g_j(\vec{x}))&#34; title=&#34;\frac{\partial g_i}{\partial x_j} = g_i(\vec{x})(\delta_{ij} - g_j(\vec{x}))&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\delta_{ij}&#34; title=&#34;\delta_{ij}&#34; /&gt; is the Kronecker delta and yields&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\delta_{ij}&amp;space;=&amp;space;\begin{cases}&amp;space;1&amp;space;&amp;\text{if&amp;space;}&amp;space;i&amp;space;=&amp;space;j&amp;space;\\&amp;space;0&amp;space;&amp;\text{if&amp;space;}&amp;space;i&amp;space;\neq&amp;space;j&amp;space;\end{cases}&#34; title=&#34;\delta_{ij} = \begin{cases} 1 &amp;\text{if } i = j \\ 0 &amp;\text{if } i \neq j \end{cases}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Jacobian of &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\vec{g}&#34; title=&#34;\vec{g}&#34; /&gt; is given by&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\mathbf{J}&amp;space;=&amp;space;\begin{bmatrix}&amp;space;\frac{\partial&amp;space;g_1}{\partial&amp;space;x_1}&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_1}{\partial&amp;space;x_2}&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_1}{\partial&amp;space;x_3}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_1}{\partial&amp;space;x_n}\\&amp;space;\frac{\partial&amp;space;g_2}{\partial&amp;space;x_1}&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_2}{\partial&amp;space;x_2}&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_2}{\partial&amp;space;x_3}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_2}{\partial&amp;space;x_n}\\&amp;space;\frac{\partial&amp;space;g_3}{\partial&amp;space;x_1}&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_3}{\partial&amp;space;x_2}&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_3}{\partial&amp;space;x_3}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_3}{\partial&amp;space;x_n}\\&amp;space;\vdots&amp;space;&amp;&amp;space;\vdots&amp;space;&amp;&amp;space;\vdots&amp;space;&amp;&amp;space;\ddots&amp;space;&amp;&amp;space;\vdots\\&amp;space;\frac{\partial&amp;space;g_n}{\partial&amp;space;x_1}&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_n}{\partial&amp;space;x_2}&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_n}{\partial&amp;space;x_3}&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;\frac{\partial&amp;space;g_n}{\partial&amp;space;x_n}&amp;space;\end{bmatrix}&#34; title=&#34;\mathbf{J} = \begin{bmatrix} \frac{\partial g_1}{\partial x_1} &amp; \frac{\partial g_1}{\partial x_2} &amp; \frac{\partial g_1}{\partial x_3} &amp; \dots &amp; \frac{\partial g_1}{\partial x_n}\\ \frac{\partial g_2}{\partial x_1} &amp; \frac{\partial g_2}{\partial x_2} &amp; \frac{\partial g_2}{\partial x_3} &amp; \dots &amp; \frac{\partial g_2}{\partial x_n}\\ \frac{\partial g_3}{\partial x_1} &amp; \frac{\partial g_3}{\partial x_2} &amp; \frac{\partial g_3}{\partial x_3} &amp; \dots &amp; \frac{\partial g_3}{\partial x_n}\\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \frac{\partial g_n}{\partial x_1} &amp; \frac{\partial g_n}{\partial x_2} &amp; \frac{\partial g_n}{\partial x_3} &amp; \dots &amp; \frac{\partial g_n}{\partial x_n} \end{bmatrix}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;which, based on the partial derivatives computed above, becomes&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\mathbf{J}&amp;space;=&amp;space;\begin{bmatrix}&amp;space;g_1(\vec{x})(1&amp;space;-&amp;space;g_1(\vec{x}))&amp;space;&amp;&amp;space;-g_1(\vec{x})g_2(\vec{x})&amp;space;&amp;&amp;space;-g_1(\vec{x})g_3(\vec{x})&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;-g_1(\vec{x})g_n(\vec{x})\\&amp;space;-g_2(\vec{x})g_1(\vec{x})&amp;space;&amp;&amp;space;g_2(\vec{x})(1&amp;space;-&amp;space;g_2(\vec{x}))&amp;space;&amp;&amp;space;-g_2(\vec{x})g_3(\vec{x})&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;-g_2(\vec{x})g_n(\vec{x})\\&amp;space;-g_3(\vec{x})g_1(\vec{x})&amp;space;&amp;&amp;space;-g_3(\vec{x})g_2(\vec{x})&amp;space;&amp;&amp;space;g_3(\vec{x})(1&amp;space;-&amp;space;g_3(\vec{x}))&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;-g_3(\vec{x})g_n(\vec{x})\\&amp;space;\vdots&amp;space;&amp;&amp;space;\vdots&amp;space;&amp;&amp;space;\vdots&amp;space;&amp;&amp;space;\ddots&amp;space;&amp;&amp;space;\vdots\\&amp;space;-g_n(\vec{x})g_1(\vec{x})&amp;space;&amp;&amp;space;-g_n(\vec{x})g_2(\vec{x})&amp;space;&amp;&amp;space;-g_n(\vec{x})g_3(\vec{x})&amp;space;&amp;&amp;space;\dots&amp;space;&amp;&amp;space;g_n(\vec{x})(1-g_n(\vec{x}))&amp;space;\end{bmatrix}&#34; title=&#34;\mathbf{J} = \begin{bmatrix} g_1(\vec{x})(1 - g_1(\vec{x})) &amp; -g_1(\vec{x})g_2(\vec{x}) &amp; -g_1(\vec{x})g_3(\vec{x}) &amp; \dots &amp; -g_1(\vec{x})g_n(\vec{x})\\ -g_2(\vec{x})g_1(\vec{x}) &amp; g_2(\vec{x})(1 - g_2(\vec{x})) &amp; -g_2(\vec{x})g_3(\vec{x}) &amp; \dots &amp; -g_2(\vec{x})g_n(\vec{x})\\ -g_3(\vec{x})g_1(\vec{x}) &amp; -g_3(\vec{x})g_2(\vec{x}) &amp; g_3(\vec{x})(1 - g_3(\vec{x})) &amp; \dots &amp; -g_3(\vec{x})g_n(\vec{x})\\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ -g_n(\vec{x})g_1(\vec{x}) &amp; -g_n(\vec{x})g_2(\vec{x}) &amp; -g_n(\vec{x})g_3(\vec{x}) &amp; \dots &amp; g_n(\vec{x})(1-g_n(\vec{x})) \end{bmatrix}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note: for multiclass classification problems, the loss function is usually the cross-entropy loss function and the activation of the output layer the softmax function. When combined together, the gradient of the cross-entropy loss function with respect to the linear activation of the output layer (that is, the activation pre-softmax) takes a very simple form as shown &lt;a href=&#34;https://srenevey.github.com/docs/losses/crossentropy/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. For this reason, the gradient of the softmax activation function in neuro returns its output and should only be used for the output layer when the cross-entropy loss function is used. This might change in the future.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Stanford CS231n course notes: &lt;a href=&#34;https://cs231n.github.io/linear-classify/#softmax&#34; target=&#34;_blank&#34;&gt;https://cs231n.github.io/linear-classify/#softmax&lt;/a&gt;, accessed November 2019.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Tanh</title>
      <link>https://srenevey.github.com/docs/activations/tanh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/activations/tanh/</guid>
      <description>
        
        
        &lt;p&gt;The hyperbolic tangent activation function is given by&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?g(x)&amp;space;=&amp;space;\tanh(x)&amp;space;=&amp;space;\frac{e^x&amp;space;-&amp;space;e^{-x}}{e^x&amp;space;&amp;plus;&amp;space;e^{-x}}&#34; title=&#34;g(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}&#34; /&gt;
&lt;img class=&#34;center&#34; src=&#34;../images/tanh.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and its derivative is&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\frac{\partial&amp;space;g}{\partial&amp;space;x}&amp;space;=&amp;space;1&amp;space;-&amp;space;\left(\frac{e^x&amp;space;-&amp;space;e^{-x}}{e^x&amp;space;&amp;plus;&amp;space;e^{-x}}&amp;space;\right)^2&amp;space;=&amp;space;1&amp;space;-&amp;space;g(x)^2&#34; title=&#34;\frac{\partial g}{\partial x} = 1 - \left(\frac{e^x - e^{-x}}{e^x + e^{-x}} \right)^2 = 1 - g(x)^2&#34; /&gt;
&lt;img class=&#34;center&#34; src=&#34;../images/dtanh.svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This activation function has a similar shape as the logisitc function except that its domain definition is centered on 0. It also has a steeper derivative at &lt;img src=&#34;https://latex.codecogs.com/svg.latex?x&amp;space;=&amp;space;0&#34; title=&#34;x = 0&#34; /&gt;. This activation function is usually used with hidden layers and is a relatively common choice.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Adam</title>
      <link>https://srenevey.github.com/docs/optimizers/adam/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/optimizers/adam/</guid>
      <description>
        
        
        

&lt;p&gt;The Adam optimizer is an adaptive learning rate algorithm and is widely used due in part to its robustness to the choice of the hyperparameters. The algorithm keeps track of the first- and second-order moment estimates for each parameter &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\theta&#34; title=&#34;\theta&#34; /&gt;, thus natively incorporating momentum (with the first-order moment). The moment estimates are initialized with &lt;img src=&#34;https://latex.codecogs.com/svg.latex?v&amp;space;=&amp;space;0&#34; title=&#34;v = 0&#34; /&gt; and &lt;img src=&#34;https://latex.codecogs.com/svg.latex?s&amp;space;=&amp;space;0&#34; title=&#34;s = 0&#34; /&gt;. At each iteration, the first moment estimate is updated with&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?v&amp;space;=&amp;space;\beta_1v&amp;space;&amp;plus;&amp;space;(1-\beta_1)\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y)&#34; title=&#34;v = \beta_1v + (1-\beta_1)\nabla_\theta\mathcal{L}(\hat{Y}, Y)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and the second one with&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?s&amp;space;=&amp;space;\beta_2s&amp;space;&amp;plus;&amp;space;(1&amp;space;-&amp;space;\beta_2)[\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y)]^2&#34; title=&#34;s = \beta_2s + (1 - \beta_2)[\nabla_\theta\mathcal{L}(\hat{Y}, Y)]^2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\beta_1&#34; title=&#34;\beta_1&#34; /&gt; and &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\beta_2&#34; title=&#34;\beta_2&#34; /&gt; are the exponential decay rates for the first and second moment estimates respectively, and the gradient of the loss function &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y)&#34; title=&#34;\nabla_\theta\mathcal{L}(\hat{Y}, Y)&#34; /&gt; is obtained from the backpropagation algorithm. Once these estimates have been computed, we correct for the bias in the first and second moments:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\hat{v}&amp;space;=&amp;space;\frac{v}{1&amp;space;-&amp;space;\beta_1^t}&#34; title=&#34;\hat{v} = \frac{v}{1 - \beta_1^t}&#34; /&gt;
&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\hat{s}&amp;space;=&amp;space;\frac{s}{1&amp;space;-&amp;space;\beta_2^t}&#34; title=&#34;\hat{s} = \frac{s}{1 - \beta_2^t}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?t&#34; title=&#34;t&#34; /&gt; is the time step and is updated after each batch has been processed. The parameter is finally updated with&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\theta&amp;space;=&amp;space;\theta&amp;space;-&amp;space;\alpha\frac{\hat{v}}{\sqrt{\hat{s}}&amp;space;&amp;plus;&amp;space;\varepsilon}&#34; title=&#34;\theta = \theta - \alpha\frac{\hat{v}}{\sqrt{\hat{s}} + \varepsilon}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\alpha&#34; title=&#34;\alpha&#34; /&gt; is the learning rate and &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\varepsilon&#34; title=&#34;\varepsilon&#34; /&gt; is a small number added for numerical stability. In neuro, the default values for the parameters of the optimizer are set based on Goodfellow et at. [1] recommendations:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\begin{align*}&amp;space;\beta_1&amp;space;&amp;=&amp;space;0.9&amp;space;\\&amp;space;\beta_2&amp;space;&amp;=&amp;space;0.999&amp;space;\\&amp;space;\varepsilon&amp;space;&amp;=&amp;space;10^{-8}&amp;space;\end{align*}&#34; title=&#34;\begin{align*} \beta_1 &amp;= 0.9 \\ \beta_2 &amp;= 0.999 \\ \varepsilon &amp;= 10^{-8} \end{align*}&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Goodfellow, I., Bengio, Y., and Courville, A., &lt;em&gt;Deep Learning&lt;/em&gt;, MIT Press, Cambridge MA, 2017&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: RMSProp</title>
      <link>https://srenevey.github.com/docs/optimizers/rmsprop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/optimizers/rmsprop/</guid>
      <description>
        
        
        

&lt;p&gt;The RMSProp optimization algorithm has first been introduced by Hinton [1]. This algorithm keeps track of the moving average of the square of the gradient:&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?s&amp;space;=&amp;space;\beta&amp;space;s&amp;space;&amp;plus;&amp;space;(1&amp;space;-&amp;space;\beta)(\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y))^2&#34; title=&#34;s = \beta s + (1 - \beta)(\nabla_\theta\mathcal{L}(\hat{Y}, Y))^2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\beta&#34; title=&#34;\beta&#34; /&gt; is the decay rate of the moving average and the gradient of the loss function &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y)&#34; title=&#34;\nabla_\theta\mathcal{L}(\hat{Y}, Y)&#34; /&gt; is computed with the backpropagation algorithm. The parameters are then updated according to&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\theta&amp;space;=&amp;space;\theta&amp;space;-&amp;space;\alpha\frac{\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y)}{\sqrt{s}&amp;space;&amp;plus;&amp;space;\varepsilon}&#34; title=&#34;\theta = \theta - \alpha\frac{\nabla_\theta\mathcal{L}(\hat{Y}, Y)}{\sqrt{s} + \varepsilon}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\alpha&#34; title=&#34;\alpha&#34; /&gt; is the learning rate and &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\varepsilon&#34; title=&#34;\varepsilon&#34; /&gt; is a small quantity added to ensure numerical stability. Note that &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\sqrt{s}&#34; title=&#34;\sqrt{s}&#34; /&gt; is applied element-wise. By default, the hyperparameters of the optimizer are set to&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\begin{align*}&amp;space;\beta&amp;space;&amp;=&amp;space;0.9&amp;space;\\&amp;space;\varepsilon&amp;space;&amp;=&amp;space;10^{-8}&amp;space;\end{align*}&#34; title=&#34;\begin{align*} \beta &amp;= 0.9 \\ \varepsilon &amp;= 10^{-8} \end{align*}&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Lecture notes: &lt;a href=&#34;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34;&gt;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&lt;/a&gt;, accessed November 2019.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: SGD</title>
      <link>https://srenevey.github.com/docs/optimizers/sgd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.com/docs/optimizers/sgd/</guid>
      <description>
        
        
        

&lt;p&gt;The stochastic gradient descent optimizer updates the parameters by following the direction of steepest descent of the gradient. That is, for parameter &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\theta&#34; title=&#34;\theta&#34; /&gt;, the update rule is&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\theta&amp;space;=&amp;space;\theta&amp;space;-&amp;space;\alpha&amp;space;\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y)&#34; title=&#34;\theta = \theta - \alpha \nabla_\theta\mathcal{L}(\hat{Y}, Y)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\alpha&#34; title=&#34;\alpha&#34; /&gt; is the learning rate and the gradient of the loss function &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y)&#34; title=&#34;\nabla_\theta\mathcal{L}(\hat{Y}, Y)&#34; /&gt; is computed with the backpropagation algorithm.&lt;/p&gt;

&lt;h4 id=&#34;stochastic-gradient-descent-with-momentum&#34;&gt;Stochastic Gradient Descent with Momentum&lt;/h4&gt;

&lt;p&gt;Training a neural network with the SGD optimizer can be slow. A method to improve the convergence rate is to introduce momentum. This method keeps track of the past gradients by storing the moving average of the gradient. The moving average (sometimes called velocity by analogy with physical kinematic systems) is initialized as &lt;img src=&#34;https://latex.codecogs.com/svg.latex?v&amp;space;=&amp;space;0&#34; title=&#34;v = 0&#34; /&gt; and is then updated according to&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?v&amp;space;=&amp;space;\beta&amp;space;v&amp;space;-&amp;space;\alpha\nabla_\theta\mathcal{L}(\hat{Y},&amp;space;Y)&#34; title=&#34;v = \beta v - \alpha\nabla_\theta\mathcal{L}(\hat{Y}, Y)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\beta&#34; title=&#34;\beta&#34; /&gt; is the exponential delay rate of the first-moment estimate (the momentum). The update rule for &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\theta&#34; title=&#34;\theta&#34; /&gt; for the SGD with momentum becomes&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;center&#34; src=&#34;https://latex.codecogs.com/svg.latex?\theta&amp;space;=&amp;space;\theta&amp;space;&amp;plus;&amp;space;v&#34; title=&#34;\theta = \theta + v&#34; /&gt;&lt;/p&gt;

&lt;p&gt;By default &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\beta&#34; title=&#34;\beta&#34; /&gt; is set to 0 in neuro&amp;rsquo;s SGD optimizer implementation. It can be turned on by creating the optimizer with the method &lt;code&gt;with_param&lt;/code&gt;. A typical value is &lt;img src=&#34;https://latex.codecogs.com/svg.latex?\beta&amp;space;=&amp;space;0.9&#34; title=&#34;\beta = 0.9&#34; /&gt; [1].&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Stanford CS231n course notes: &lt;a href=&#34;https://cs231n.github.io/neural-networks-3/#sgd&#34; target=&#34;_blank&#34;&gt;https://cs231n.github.io/neural-networks-3/#sgd&lt;/a&gt;, accessed November 2019.&lt;/p&gt;

&lt;p&gt;[2] Goodfellow, I., Bengio, Y., and Courville, A., &lt;em&gt;Deep Learning&lt;/em&gt;, MIT Press, Cambridge MA, 2017&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
