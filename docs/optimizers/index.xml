<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neuro â€“ Optimizers</title>
    <link>https://srenevey.github.io/neuro/docs/optimizers/</link>
    <description>Recent content in Optimizers on neuro</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://srenevey.github.io/neuro/docs/optimizers/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Adadelta</title>
      <link>https://srenevey.github.io/neuro/docs/optimizers/adadelta/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.io/neuro/docs/optimizers/adadelta/</guid>
      <description>
        
        
        

&lt;p&gt;This optimizer has first been introduced by Zeiler [1] in order to address two issues that arise in the fomulation of the Adagrad optimizer. In the Adagrad optimizer, the update rule is&lt;/p&gt;

&lt;p&gt;$$x_{t+1} = x_t + \Delta x_t = x_t -\frac{\eta}{\sqrt{\sum_{\tau=1}^t g_t^2}}g_t$$&lt;/p&gt;

&lt;p&gt;where $x_t$ is the parameter at step $t$, $\eta$ the learning rate, and $g_t$ the gradient. We see from this equation that if the value of the gradient is large at the beginning of the simulation, the learning rate (the term in front of $g_t$) will be small for the entire training since the denominator will remain large no matter how small the gradient becomes. This can be addressed by selecting a large $\eta$ but thus makes the method dependent on the choice of the learning rate. Also, since the gradient keeps accumulating in the denominator, the learning rate will continually decrease and eventually become zero, hence stopping the training. Adadelta was designed to address these two properties of Adagrad.&lt;/p&gt;

&lt;p&gt;The Adadelta optimizer keeps track of only recent past gradients by accumulating them using an exponentially decaying average:&lt;/p&gt;

&lt;p&gt;$$E[g^2]_{t} = \rho E[g^2]_{t-1} + (1-\rho)g_t^2$$&lt;/p&gt;

&lt;p&gt;where $\rho$ is the exponential decay rate. That way, large gradients that may arise at the beginning of the training are slowly &amp;ldquo;forgotten&amp;rdquo; as the training progresses and thus don&amp;rsquo;t impact the learning rate later on in the optimization. Since in Adagrad the denominator is a square root, the root mean square is computed:&lt;/p&gt;

&lt;p&gt;$$RMS[g]_{t} = \sqrt{E[g^2]_t + \epsilon}$$&lt;/p&gt;

&lt;p&gt;where $\epsilon$ is a small constant added for numerical stability. The derivation of an expression for the numerator is based on the observation that the units of $x_t$ and $\Delta x_t$ should match and based on considerations on second order methods, the author proposes the following expression:&lt;/p&gt;

&lt;p&gt;$$E[\Delta x^2]_{t} = \rho E[\Delta x^2]_{t-1} + (1-\rho)\Delta x_t^2$$&lt;/p&gt;

&lt;p&gt;$$RMS[\Delta x]_t = \sqrt{E[\Delta x^2]_t + \epsilon}$$&lt;/p&gt;

&lt;p&gt;Hence, the update term in the Adadelta method is&lt;/p&gt;

&lt;p&gt;$$\Delta x_t = - \frac{RMS[g]_t}{RMS[\Delta x]_t}g_t$$&lt;/p&gt;

&lt;p&gt;In neuro, the default values for $\rho$ and $\epsilon$ are&lt;/p&gt;

&lt;p&gt;$$\rho = 0.95$$&lt;/p&gt;

&lt;p&gt;$$\epsilon = 10^{-6}$$&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Zeiler, M.D., Adadelta: An Adaptive Learning Rate Method, &lt;a href=&#34;https://arxiv.org/abs/1212.5701&#34; target=&#34;_blank&#34;&gt;arXiv:1212.5701v1&lt;/a&gt;, 2012.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Adam</title>
      <link>https://srenevey.github.io/neuro/docs/optimizers/adam/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.io/neuro/docs/optimizers/adam/</guid>
      <description>
        
        
        

&lt;p&gt;The Adam optimizer is an adaptive learning rate algorithm and is widely used due in part to its robustness to the choice of the hyperparameters. The algorithm keeps track of the first- and second-order moment estimates for each parameter $\theta$, thus natively incorporating momentum (with the first-order moment). The moment estimates are initialized with $v=0$ and $s=0$. At each iteration, the first moment estimate is updated with&lt;/p&gt;

&lt;p&gt;$$v = \beta_1v+(1-\beta_1)\nabla_{\theta}\mathcal{L}(\hat{Y},Y)$$&lt;/p&gt;

&lt;p&gt;and the second one with&lt;/p&gt;

&lt;p&gt;$$s = \beta_2s + (1-\beta_2)[\nabla_{\theta}\mathcal{L}(\hat{Y}, Y)]^2$$&lt;/p&gt;

&lt;p&gt;where $\beta_1$ and $\beta_2$ are the exponential decay rates for the first and second moment estimates respectively, and the gradient of the loss function $\nabla_{\theta}\mathcal{L}(\hat{Y},Y)$ is obtained from the backpropagation algorithm. Once these estimates have been computed, we correct for the bias in the first and second moments:&lt;/p&gt;

&lt;p&gt;$$\hat{v}=\frac{v}{1-\beta_1^t}$$&lt;/p&gt;

&lt;p&gt;$$\hat{s} = \frac{s}{1-\beta_2^t}$$&lt;/p&gt;

&lt;p&gt;where $t$ is the time step and is updated after each batch has been processed. The parameter is finally updated with&lt;/p&gt;

&lt;p&gt;$$\theta = \theta - \alpha \frac{\hat{v}}{\sqrt{\hat{s}}+\epsilon}$$&lt;/p&gt;

&lt;p&gt;where $\alpha$ is the learning rate and $\epsilon$ is a small number added for numerical stability. In neuro, the default values for the parameters of the optimizer are set based on Goodfellow et at. [1] recommendations:&lt;/p&gt;

&lt;p&gt;$$\begin{align} \beta_1 &amp;amp;= 0.9 \newline \beta_2 &amp;amp;= 0.999 \newline \epsilon &amp;amp;= 10^{-8} \end{align}$$&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Goodfellow, I., Bengio, Y., and Courville, A., &lt;em&gt;Deep Learning&lt;/em&gt;, MIT Press, Cambridge MA, 2017&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: RMSProp</title>
      <link>https://srenevey.github.io/neuro/docs/optimizers/rmsprop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.io/neuro/docs/optimizers/rmsprop/</guid>
      <description>
        
        
        

&lt;p&gt;The RMSProp optimization algorithm has first been introduced by Hinton [1]. This algorithm keeps track of the moving average of the square of the gradient:&lt;/p&gt;

&lt;p&gt;$$s = \beta s + (1-\beta)[\nabla_{\theta}\mathcal{L}(\hat{Y},Y)]^2$$&lt;/p&gt;

&lt;p&gt;where $\beta$ is the decay rate of the moving average and the gradient of the loss function $\nabla_{\theta}\mathcal{L}(\hat{Y},Y)$ is computed with the backpropagation algorithm. The parameters are then updated according to&lt;/p&gt;

&lt;p&gt;$$\theta = \theta - \alpha \frac{\nabla_{\theta}\mathcal{L}(\hat{Y},Y)}{\sqrt{s}+\epsilon}$$&lt;/p&gt;

&lt;p&gt;where $\alpha$ is the learning rate and $\epsilon$ is a small quantity added to ensure numerical stability. Note that $\sqrt{s}$ is applied element-wise. By default, the hyperparameters of the optimizer are set to&lt;/p&gt;

&lt;p&gt;$$\begin{align} \beta &amp;amp;= 0.9 \newline \epsilon &amp;amp;= 10^{-8} \end{align} $$&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Lecture notes: &lt;a href=&#34;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&#34; target=&#34;_blank&#34;&gt;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&lt;/a&gt;, accessed November 2019.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: SGD</title>
      <link>https://srenevey.github.io/neuro/docs/optimizers/sgd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.io/neuro/docs/optimizers/sgd/</guid>
      <description>
        
        
        

&lt;p&gt;The stochastic gradient descent optimizer updates the parameters by following the direction of steepest descent of the gradient. That is, for parameter $\theta$, the update rule is&lt;/p&gt;

&lt;p&gt;$$\theta = \theta - \alpha \nabla_{\theta}\mathcal{L}(\hat{Y},Y)$$&lt;/p&gt;

&lt;p&gt;where $\alpha$ is the learning rate and the gradient of the loss function $\nabla_{\theta}\mathcal{L}(\hat{Y},Y)$ is computed with the backpropagation algorithm.&lt;/p&gt;

&lt;h4 id=&#34;stochastic-gradient-descent-with-momentum&#34;&gt;Stochastic Gradient Descent with Momentum&lt;/h4&gt;

&lt;p&gt;Training a neural network with the SGD optimizer can be slow. A method to improve the convergence rate is to introduce momentum. This method keeps track of the past gradients by storing the moving average of the gradient. The moving average (sometimes called velocity by analogy with physical kinematic systems) is initialized as $v=0$ and is then updated according to&lt;/p&gt;

&lt;p&gt;$$v = \beta v - \alpha \nabla_{\theta}\mathcal{L}(\hat{Y}, Y)$$&lt;/p&gt;

&lt;p&gt;where $\beta$ is the exponential delay rate of the first-moment estimate (the momentum). The update rule for $\theta$ for the SGD with momentum becomes&lt;/p&gt;

&lt;p&gt;$$\theta = \theta + v$$&lt;/p&gt;

&lt;p&gt;By default $\beta$ is set to 0 in neuro&amp;rsquo;s SGD optimizer implementation. It can be turned on by creating the optimizer with the method &lt;code&gt;with_param&lt;/code&gt;. A typical value is $\beta = 0.9$ [1].&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;[1] Stanford CS231n course notes: &lt;a href=&#34;https://cs231n.github.io/neural-networks-3/#sgd&#34; target=&#34;_blank&#34;&gt;https://cs231n.github.io/neural-networks-3/#sgd&lt;/a&gt;, accessed November 2019.&lt;/p&gt;

&lt;p&gt;[2] Goodfellow, I., Bengio, Y., and Courville, A., &lt;em&gt;Deep Learning&lt;/em&gt;, MIT Press, Cambridge MA, 2017&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
