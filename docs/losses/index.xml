<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neuro â€“ Losses</title>
    <link>https://srenevey.github.io/neuro/docs/losses/</link>
    <description>Recent content in Losses on neuro</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://srenevey.github.io/neuro/docs/losses/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Cross Entropy</title>
      <link>https://srenevey.github.io/neuro/docs/losses/crossentropy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.io/neuro/docs/losses/crossentropy/</guid>
      <description>
        
        
        &lt;p&gt;The cross entropy loss function is typically used in multiclass classification problems. For a model having $C$ classes and if $\hat{Y} = \begin{bmatrix} \boldsymbol{\hat{y}}^{(1)} &amp;amp; \boldsymbol{\hat{y}}^{(2)} &amp;amp; \dots &amp;amp; \boldsymbol{\hat{y}}^{(m)} \end{bmatrix}$ are the predicted labels and $Y = \begin{bmatrix} \boldsymbol{y}^{(1)} &amp;amp; \boldsymbol{y}^{(2)} &amp;amp; \dots &amp;amp; \boldsymbol{y}^{(m)} \end{bmatrix}$ are the true labels, then the cross entropy is defined as&lt;/p&gt;

&lt;p&gt;$$\mathcal{L}(\hat{Y}, Y) = \frac{1}{m}\sum_{i=1}^m\left(-\sum_{j=1}^Cy^{(i)}_j\log\hat{y}^{(i)}_j \right)$$&lt;/p&gt;

&lt;p&gt;where $m$ is the number of samples in the mini-batch. If we take the derivative of this function with respect to output $j$ of sample $i$ we get&lt;/p&gt;

&lt;p&gt;$$\frac{\partial \mathcal{L}}{\partial \hat{y}_j^{(i)}} = -\frac{1}{m}\frac{y_j^{(i)}}{\hat{y}_j^{(i)}}$$
or in vector form&lt;/p&gt;

&lt;p&gt;$$\nabla_{\hat{Y}}\mathcal{L}(\hat{Y},Y) = -\frac{1}{m}\frac{Y}{\hat{Y }}$$
Now, if we assume that the loss activation of the last layer is a softmax function, we have&lt;/p&gt;

&lt;p&gt;$$\hat{y}_j^{(i)} = \frac{e^{x_j^{(i)}}}{\sum_{k=1}^ne^{x_k^{(i)}}}$$&lt;/p&gt;

&lt;p&gt;and, as seen in the derivation of the &lt;a href=&#34;https://srenevey.github.io/neuro/docs/activations/softmax/&#34; target=&#34;_blank&#34;&gt;softmax&lt;/a&gt;, the derivative is&lt;/p&gt;

&lt;p&gt;$$\frac{\partial \hat{y}_j^{(i)}}{\partial x_l^{(i)}} = \hat{y}_j^{(i)}(\delta_{jl} - \hat{y}_l^{(i)})$$&lt;/p&gt;

&lt;p&gt;Hence, if we compute the derivative of $\mathcal{L}$ with respect to $x_l^{(i)}$, we have&lt;/p&gt;

&lt;p&gt;$$\frac{\partial \mathcal{L}}{\partial x_l^{(i)}} = \sum_{k=1}^C\frac{\partial \mathcal{L}}{\partial \hat{y}_k^{(i)}}\frac{\partial \hat{y}_k^{(i)}}{\partial x_l^{(i)}}$$&lt;/p&gt;

&lt;p&gt;where $C$ is the number of classes. If we replace the two fractions on the right hand side by the equations found previously, we have&lt;/p&gt;

&lt;p&gt;$$\frac{\partial \mathcal{L}}{\partial x_l^{(i)}} = -\sum_{k=1}^C\frac{y_k^{(i)}}{\hat{y}_k^{(i)}}\hat{y}_k^{(i)}(\delta_{kl} - \hat{y}_l^{(i)}) = -\sum_{k=1}^Cy_k^{(i)}(\delta_{kl} - \hat{y}_l^{(i)})$$&lt;/p&gt;

&lt;p&gt;Separating the cases $l=k$ and $l \neq k$ yields&lt;/p&gt;

&lt;p&gt;$$\frac{\partial \mathcal{L}}{\partial x_l^{(i)}} = -y_l^{(i)}(1 - \hat{y}_l^{(i)}) + \sum_{k\neq l}^Cy_k^{(i)}\hat{y}_l^{(i)} = \hat{y}_l^{(i)}\left(y_l^{(i)} + \sum_{k\neq l}^Cy_k^{(i)} \right ) - y_l^{(i)}$$
Since the true label are one-hot encoded, we have&lt;/p&gt;

&lt;p&gt;$$y_l^{(i)} + \sum_{k\neq l}^Cy_k^{(i)} = 1$$
so that&lt;/p&gt;

&lt;p&gt;$$\frac{\partial \mathcal{L}}{\partial x_l^{(i)}} = \hat{y}_l^{(i)} - y_l^{(i)}$$&lt;/p&gt;

&lt;p&gt;We see therefore that if the activation of the last layer is a softmax function, the gradient of the cross-entropy loss with respect to the linear activation of the last layer takes the simple form&lt;/p&gt;

&lt;p&gt;$$\nabla_X\mathcal{L}(\hat{Y}, Y) = \frac{1}{m}(\hat{Y} - Y)$$&lt;/p&gt;

&lt;p&gt;which can be easily computed. For this reason, the current implementation of the cross-entropy loss function in neuro assumes that the last activation is a softmax function. Similarly, the softmax activation function can only be used for the last layer of the network with the cross-entropy loss function. This might change in the future.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Mean Absolute Error</title>
      <link>https://srenevey.github.io/neuro/docs/losses/mae/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.io/neuro/docs/losses/mae/</guid>
      <description>
        
        
        &lt;p&gt;The mean absolute error loss function computes the mean of the absolute values of the error. Assuming that the predicted values are $\hat{Y} = \begin{bmatrix} \boldsymbol{\hat{y}}^{(1)} &amp;amp; \boldsymbol{\hat{y}}^{(2)} &amp;amp; \dots &amp;amp; \boldsymbol{\hat{y}}^{(m)} \end{bmatrix}$ and that the true values are $Y = \begin{bmatrix} \boldsymbol{y}^{(1)} &amp;amp; \boldsymbol{y}^{(2)} &amp;amp; \dots &amp;amp; \boldsymbol{y}^{(m)} \end{bmatrix}$, then the mean absolute error is given by&lt;/p&gt;

&lt;p&gt;$$\mathcal{L}(\hat{Y},Y)=\frac{1}{m}\sum_{i=1}^m|\boldsymbol{\hat{y}}^{(i)}-\boldsymbol{y}^{(i)}|$$&lt;/p&gt;

&lt;p&gt;where $m$ is the number of samples in the mini-batch. Since the derivative of the absolute value is given by&lt;/p&gt;

&lt;p&gt;$$\frac{d|x|}{dx} = \frac{x}{|x|} = \begin{cases} 1 &amp;amp; \text{if } x &amp;gt; 0 \newline \text{undefined} &amp;amp; \text{if } x = 0 \newline -1 &amp;amp; \text{if } x &amp;lt; 0 \end{cases}$$&lt;/p&gt;

&lt;p&gt;the gradient of $\mathcal{L}$ with respect to $\hat{Y}$ yields&lt;/p&gt;

&lt;p&gt;$$\nabla_{\hat{Y}}\mathcal{L}(\hat{Y}, Y)=\begin{cases}\frac{1}{m} &amp;amp;\text{if } \boldsymbol{\hat{y}}^{(i)} \ge \boldsymbol{y}^{(i)} \newline -\frac{1}{m} &amp;amp;\text{if } \boldsymbol{\hat{y}}^{(i)} &amp;lt; \boldsymbol{y}^{(i)} \end{cases}$$&lt;/p&gt;

&lt;p&gt;where the case $\boldsymbol{\hat{y}}^{(i)} = \boldsymbol{y}^{(i)}$ is arbitrarily set to $\frac{1}{m}$.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Mean Squared Error</title>
      <link>https://srenevey.github.io/neuro/docs/losses/mse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://srenevey.github.io/neuro/docs/losses/mse/</guid>
      <description>
        
        
        &lt;p&gt;The mean squared error loss function computes the mean of the square of the error of the model. If the model predicts $\hat{Y} = \begin{bmatrix} \boldsymbol{\hat{y}}^{(1)} &amp;amp; \boldsymbol{\hat{y}}^{(2)} &amp;amp; \dots &amp;amp; \boldsymbol{\hat{y}}^{(m)} \end{bmatrix}$ and the true values are $Y = \begin{bmatrix} \boldsymbol{y}^{(1)} &amp;amp; \boldsymbol{y}^{(2)} &amp;amp; \dots &amp;amp; \boldsymbol{y}^{(m)} \end{bmatrix}$, then  the mean squared error is computed as&lt;/p&gt;

&lt;p&gt;$$\mathcal{L}(\hat{Y},Y) = \frac{1}{m}\sum_{i=1}^m\Vert\boldsymbol{\hat{y}}^{(i)}-\boldsymbol{y}^{(i)}\Vert^2$$&lt;/p&gt;

&lt;p&gt;where $m$ is the number of samples in the mini-batch. Taking the gradient of this function with respect to the predicted values yields&lt;/p&gt;

&lt;p&gt;$$\nabla_{\hat{Y}}\mathcal{L}(\hat{Y},Y)=\frac{2}{m}(\hat{Y}-Y)$$&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
